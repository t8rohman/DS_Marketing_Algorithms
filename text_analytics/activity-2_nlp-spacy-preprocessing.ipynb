{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building A More Complex Preprocessing Pipeline: Reddit Self-Posts Dataset\n",
    "\n",
    "In this notebook, I will build a more complex preprocessing pipeline. Functions referred to Blueprints for Text Analytics by Albrecht et al. (2021) with several adjustments to make it more clear. For the ready-to-use functions, please refer to file **fun_nlp_spacy_text.py.** There are several steps at least until we get our prepared text:\n",
    "\n",
    "1. Source Text (The process is called **Data Cleaning**)\n",
    "    - Identify noise\n",
    "    - Noise removal\n",
    "    - Character normalization\n",
    "    - Data masking\n",
    "2. Clean Text (The process is called **Linguistic Process**)\n",
    "    - Tokenization\n",
    "    - POS taggin\n",
    "    - Lemmatization\n",
    "    - Named-entity recognition\n",
    "3. Prepared Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sqlite3  # to save dataframe\n",
    "import html\n",
    "import spacy\n",
    "\n",
    "import regex as re  # regular expression\n",
    "import nltk  # library that contains stopwords, especially in english\n",
    "import itertools  # to iterate items in set\n",
    "import textacy.preprocessing as tprep\n",
    "import fun_preprocessing_text as pp_text_1\n",
    "from textacy.extract.kwic import keyword_in_context as KWIC  # to make keyword-in-context (KWIC) analysis\n",
    "from wordcloud import WordCloud  # to make word clouds\n",
    "\n",
    "from collections import Counter  # to count list contains, similar to value_counts(), but faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>95344</th>\n",
       "      <td>6gpomc</td>\n",
       "      <td>VietNam</td>\n",
       "      <td>Going back to Vietname - with 2 passports - wi...</td>\n",
       "      <td>Hi,&lt;lb&gt;&lt;lb&gt; I have a question regarding VISA a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55748</th>\n",
       "      <td>737mhl</td>\n",
       "      <td>ibs</td>\n",
       "      <td>At wits end pain</td>\n",
       "      <td>Hi all. I'm a 28 year old male suffering from ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>778318</th>\n",
       "      <td>7j2w15</td>\n",
       "      <td>lightingdesign</td>\n",
       "      <td>Budget lighting brands</td>\n",
       "      <td>I've recently taken a position as TD at a larg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180137</th>\n",
       "      <td>75b2mj</td>\n",
       "      <td>ASUS</td>\n",
       "      <td>Getting \"5-Way Optimization\" to work?</td>\n",
       "      <td>Is there a way to get 5-Way Optimization to ac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497414</th>\n",
       "      <td>4rcny7</td>\n",
       "      <td>Rainmeter</td>\n",
       "      <td>Are there any complete themes/skins available ...</td>\n",
       "      <td>I'm looking to start using Rainmeter, and I th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id       subreddit  \\\n",
       "95344   6gpomc         VietNam   \n",
       "55748   737mhl             ibs   \n",
       "778318  7j2w15  lightingdesign   \n",
       "180137  75b2mj            ASUS   \n",
       "497414  4rcny7       Rainmeter   \n",
       "\n",
       "                                                    title  \\\n",
       "95344   Going back to Vietname - with 2 passports - wi...   \n",
       "55748                                    At wits end pain   \n",
       "778318                             Budget lighting brands   \n",
       "180137              Getting \"5-Way Optimization\" to work?   \n",
       "497414  Are there any complete themes/skins available ...   \n",
       "\n",
       "                                                 selftext  \n",
       "95344   Hi,<lb><lb> I have a question regarding VISA a...  \n",
       "55748   Hi all. I'm a 28 year old male suffering from ...  \n",
       "778318  I've recently taken a position as TD at a larg...  \n",
       "180137  Is there a way to get 5-Way Optimization to ac...  \n",
       "497414  I'm looking to start using Rainmeter, and I th...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataframe: 1013000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>category_1</th>\n",
       "      <th>category_2</th>\n",
       "      <th>category_3</th>\n",
       "      <th>in_data</th>\n",
       "      <th>reason_for_exclusion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2436</th>\n",
       "      <td>chelseafc</td>\n",
       "      <td>sports</td>\n",
       "      <td>soccer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>fewer posts than r/soccer which shares topic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1862</th>\n",
       "      <td>bugs</td>\n",
       "      <td>programming</td>\n",
       "      <td>broad</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>too_broad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2384</th>\n",
       "      <td>nyjets</td>\n",
       "      <td>sports</td>\n",
       "      <td>football</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>fewer posts than r/oaklandraiders which shares...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1421</th>\n",
       "      <td>minimalism</td>\n",
       "      <td>hobby</td>\n",
       "      <td>interior design</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1625</th>\n",
       "      <td>namenerds</td>\n",
       "      <td>parenting</td>\n",
       "      <td>naming</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       subreddit   category_1       category_2 category_3  in_data  \\\n",
       "2436   chelseafc       sports           soccer        NaN    False   \n",
       "1862        bugs  programming            broad        NaN    False   \n",
       "2384      nyjets       sports         football        NaN    False   \n",
       "1421  minimalism        hobby  interior design        NaN     True   \n",
       "1625   namenerds    parenting           naming        NaN     True   \n",
       "\n",
       "                                   reason_for_exclusion  \n",
       "2436       fewer posts than r/soccer which shares topic  \n",
       "1862                                          too_broad  \n",
       "2384  fewer posts than r/oaklandraiders which shares...  \n",
       "1421                                                NaN  \n",
       "1625                                                NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataframe: 3394\n"
     ]
    }
   ],
   "source": [
    "# read the data from csv and tsv\n",
    "\n",
    "df_posts = pd.read_csv('dataset/dataset_reddit/rspct.tsv', sep='\\t')\n",
    "df_subreddit = pd.read_csv('dataset/dataset_reddit/subreddit_info.csv')\n",
    "\n",
    "display(df_posts.sample(5))\n",
    "print('length of dataframe:', len(df_posts))\n",
    "\n",
    "display(df_subreddit.sample(5))\n",
    "print('length of dataframe:', len(df_subreddit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>category_1</th>\n",
       "      <th>category_2</th>\n",
       "      <th>category_3</th>\n",
       "      <th>in_data</th>\n",
       "      <th>reason_for_exclusion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>281225</th>\n",
       "      <td>7xi8y9</td>\n",
       "      <td>haskell</td>\n",
       "      <td>Joining multiple ThreadId in main thread</td>\n",
       "      <td>Once you create sparks/threads with the `forkI...</td>\n",
       "      <td>programming</td>\n",
       "      <td>haskell</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255196</th>\n",
       "      <td>782vy5</td>\n",
       "      <td>volt</td>\n",
       "      <td>Safety engine noise</td>\n",
       "      <td>Is there an option/device that will turn on si...</td>\n",
       "      <td>autos</td>\n",
       "      <td>chevrolet</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122760</th>\n",
       "      <td>6zgvn5</td>\n",
       "      <td>Rabbits</td>\n",
       "      <td>Today is the first day that Mochi has the run ...</td>\n",
       "      <td>Little by slowly we have been giving Mochi mor...</td>\n",
       "      <td>animals</td>\n",
       "      <td>rabbits</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501505</th>\n",
       "      <td>88mgyq</td>\n",
       "      <td>reloading</td>\n",
       "      <td>Whats a good cheap powder for reloading handgu...</td>\n",
       "      <td>I would like some cheap powder for reloading 9...</td>\n",
       "      <td>hardware/tools</td>\n",
       "      <td>guns</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337225</th>\n",
       "      <td>7ge08v</td>\n",
       "      <td>Wizard101</td>\n",
       "      <td>Anyway to lift a ban?</td>\n",
       "      <td>Aight so basically when i was a really young k...</td>\n",
       "      <td>video_game</td>\n",
       "      <td>wizard 101</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id  subreddit                                              title  \\\n",
       "281225  7xi8y9    haskell           Joining multiple ThreadId in main thread   \n",
       "255196  782vy5       volt                                Safety engine noise   \n",
       "122760  6zgvn5    Rabbits  Today is the first day that Mochi has the run ...   \n",
       "501505  88mgyq  reloading  Whats a good cheap powder for reloading handgu...   \n",
       "337225  7ge08v  Wizard101                              Anyway to lift a ban?   \n",
       "\n",
       "                                                 selftext      category_1  \\\n",
       "281225  Once you create sparks/threads with the `forkI...     programming   \n",
       "255196  Is there an option/device that will turn on si...           autos   \n",
       "122760  Little by slowly we have been giving Mochi mor...         animals   \n",
       "501505  I would like some cheap powder for reloading 9...  hardware/tools   \n",
       "337225  Aight so basically when i was a really young k...      video_game   \n",
       "\n",
       "        category_2 category_3  in_data reason_for_exclusion  \n",
       "281225     haskell        NaN     True                  NaN  \n",
       "255196   chevrolet        NaN     True                  NaN  \n",
       "122760     rabbits        NaN     True                  NaN  \n",
       "501505        guns        NaN     True                  NaN  \n",
       "337225  wizard 101        NaN     True                  NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataframe: 1013000\n"
     ]
    }
   ],
   "source": [
    "df = df_posts.merge(df_subreddit, \n",
    "                    left_on='subreddit', \n",
    "                    right_on='subreddit', \n",
    "                    how='inner'\n",
    "                    )\n",
    "\n",
    "display(df.sample(5))\n",
    "print('length of dataframe:', len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'subreddit', 'title', 'text', 'category', 'sub_category',\n",
       "       'sub2_category'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cleaning the name of the column\n",
    "df = df.rename(columns={\n",
    "    'selftext': 'text',\n",
    "    'category_1': 'category',\n",
    "    'category_2': 'sub_category',\n",
    "    'category_3': 'sub2_category',\n",
    "}\n",
    ")\n",
    "\n",
    "df = df.loc[:, ~df.columns.isin(['in_data', 'reason_for_exclusion'])]\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['writing/stories', 'tv_show', 'autos', 'hardware/tools',\n",
       "       'electronics', 'video_game', 'crypto', 'sports', 'hobby',\n",
       "       'appearance', 'card_game', 'drugs', 'advice/question',\n",
       "       'social_group', 'anime/manga', 'sex/relationships', 'software',\n",
       "       'health', 'other', 'animals', 'arts', 'programming', 'rpg',\n",
       "       'books', 'parenting', 'education', 'company/website', 'profession',\n",
       "       'music', 'politics/viewpoint', 'stem', 'travel', 'geo',\n",
       "       'religion/supernatural', 'board_game', 'movies', 'food/drink',\n",
       "       'finance/money', 'meta'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.category.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>132526</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <td>8449l5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subreddit</th>\n",
       "      <td>CaptainTsubasaDT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>title</th>\n",
       "      <td>Global News 2018/03/13: Dream Collection 3 in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text</th>\n",
       "      <td>**New Gacha Banner: Dream Collection 3 in 1, H...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>category</th>\n",
       "      <td>anime/manga</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sub_category</th>\n",
       "      <td>captain tsubasa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sub2_category</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                          132526\n",
       "id                                                        8449l5\n",
       "subreddit                                       CaptainTsubasaDT\n",
       "title          Global News 2018/03/13: Dream Collection 3 in ...\n",
       "text           **New Gacha Banner: Dream Collection 3 in 1, H...\n",
       "category                                             anime/manga\n",
       "sub_category                                     captain tsubasa\n",
       "sub2_category                                                NaN"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataframe: 26000\n"
     ]
    }
   ],
   "source": [
    "# we will focus our analysis only on anime/manga category\n",
    "\n",
    "df = df[df['category'] == 'anime/manga']\n",
    "\n",
    "display(df.sample(1).T)\n",
    "print('length of dataframe:', len(df))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint - Save Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data frame as the checkpoint\n",
    "# two ways: either as pickle or as SQL database as well\n",
    "\n",
    "df.to_pickle('df_reddit.pkl')  # can only be read by python, but fast and efficient\n",
    "\n",
    "db_name = 'reddit-selfposts.db'\n",
    "con = sqlite3.connect(db_name)\n",
    "df.to_sql('anime-posts', con, index=False, if_exists='replace')\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Text Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Identify Noise with RegEx\n",
    "    \n",
    "It's difficult to identify all noises in a big data. Thus, the proper way is to calculate the impurity by counting how many characters that are not plan text and may therefore disturb further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the pattern using regex first\n",
    "# IMPORTANT! adjust the patter based on your needs\n",
    "\n",
    "RE_SUSPICIOUS = re.compile(r'[&#<>{}\\[\\]\\\\]')  \n",
    "\n",
    "# define the function to calculate impurity\n",
    "\n",
    "def impurity(text, min_len=10):\n",
    "    ''''calculate the impurity of the text\n",
    "    \n",
    "    Parameters:\n",
    "    text    : text that we want to analyze\n",
    "    min_len : minimal length of the text, else it will return 0\n",
    "    \n",
    "    Returns\n",
    "    impurity level of the text\n",
    "    '''\n",
    "    if text == None or len(text) < min_len:\n",
    "        return 0\n",
    "    else:\n",
    "        return len(RE_SUSPICIOUS.findall(text))/len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "impurity score: 0.007692307692307693\n",
      "[\"I finished both endings of UBW yet in the gallery (already finished Fate's route before it of course and got all the CG except the one found in HF), I see that the CG in the 4th page, 3rd row, 3rd line is missing. Any clue for which scene is it related to?\"]\n"
     ]
    }
   ],
   "source": [
    "# test the function to one of the text\n",
    "\n",
    "temp_text = str(df.sample(1)['text'].values)\n",
    "\n",
    "print('impurity score:', impurity(temp_text))\n",
    "print(temp_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>impurity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>737246</th>\n",
       "      <td>7umots</td>\n",
       "      <td>The End is approaching&lt;lb&gt;&lt;lb&gt;&lt;lb&gt;&lt;lb&gt;&lt;lb&gt;&lt;lb&gt;...</td>\n",
       "      <td>0.288783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737556</th>\n",
       "      <td>8lanvf</td>\n",
       "      <td>Darkness creeping in, &lt;lb&gt;&lt;lb&gt;&lt;lb&gt;It’s hard to...</td>\n",
       "      <td>0.194305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737076</th>\n",
       "      <td>7v5kmd</td>\n",
       "      <td>&lt;lb&gt;&lt;lb&gt;&lt;lb&gt;Scars on my body, &lt;lb&gt;&lt;lb&gt;&lt;lb&gt;From...</td>\n",
       "      <td>0.188746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737763</th>\n",
       "      <td>7s6mvn</td>\n",
       "      <td>All right it's been two consecutive nights of ...</td>\n",
       "      <td>0.187552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797544</th>\n",
       "      <td>86r70a</td>\n",
       "      <td>-1-&lt;lb&gt;&lt;lb&gt;Welcome to Republic City&lt;lb&gt;&lt;lb&gt;Reb...</td>\n",
       "      <td>0.184953</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                                               text  impurity\n",
       "737246  7umots  The End is approaching<lb><lb><lb><lb><lb><lb>...  0.288783\n",
       "737556  8lanvf  Darkness creeping in, <lb><lb><lb>It’s hard to...  0.194305\n",
       "737076  7v5kmd  <lb><lb><lb>Scars on my body, <lb><lb><lb>From...  0.188746\n",
       "737763  7s6mvn  All right it's been two consecutive nights of ...  0.187552\n",
       "797544  86r70a  -1-<lb><lb>Welcome to Republic City<lb><lb>Reb...  0.184953"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try the function to our data frame\n",
    "# get the top 5 records with the highest impurity\n",
    "\n",
    "df['impurity'] = df['text'].apply(impurity, min_len=10)\n",
    "df[['id', 'text', 'impurity']].sort_values('impurity', ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>&lt;lb&gt;</th>\n",
       "      <td>173195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;tab&gt;</th>\n",
       "      <td>549</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         freq\n",
       "token        \n",
       "<lb>   173195\n",
       "<tab>     549"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# most of the common words are the HTML tags, which are <lb> and <tab>\n",
    "# thus, we're going to count how many the appearences using the function built from previous activity\n",
    "\n",
    "pp_text_1.count_words(df, column='text', preprocess=lambda t: re.findall(r'<[\\w/]*>', t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    '''Blueprint function first substitutes all HTML escapes (e.g., &amp;) \n",
    "    by their plain-text representation and then replaces certain patterns by spaces. \n",
    "    Finally, sequences of whitespaces are pruned.\n",
    "    \n",
    "    Taken directly from the book.\n",
    "    \n",
    "    IMPORTANT! Make some adjustment for this function based on the text you have!'''\n",
    "    \n",
    "    # convert html escapes like &amp; to characters.\n",
    "    text = html.unescape(text)\n",
    "    # tags like <tab>\n",
    "    text = re.sub(r'<[^<>]*>', ' ', text)\n",
    "    # markdown URLs like [Some text](https://....)\n",
    "    text = re.sub(r'\\[([^\\[\\]]*)\\]\\([^\\(\\)]*\\)', r'\\1', text)\n",
    "    # text or code in brackets like [0]\n",
    "    text = re.sub(r'\\[[^\\[\\]]*\\]', ' ', text)\n",
    "    # standalone sequences of specials, matches &# but not #cool\n",
    "    text = re.sub(r'(?:^|\\s)[&#<>{}\\[\\]+|\\\\:-]{1,}(?:\\s|$)', ' ', text) # standalone sequences of hyphens like --- or ==\n",
    "    text = re.sub(r'(?:^|\\s)[\\-=\\+]{2,}(?:\\s|$)', ' ', text)\n",
    "    # sequences of white spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before\n",
      "[\"I finished both endings of UBW yet in the gallery (already finished Fate's route before it of course and got all the CG except the one found in HF), I see that the CG in the 4th page, 3rd row, 3rd line is missing. Any clue for which scene is it related to?\"]\n",
      "after\n",
      "\n",
      "_\n",
      "impurity score before eliminating HTML tags: 0.007692307692307693\n",
      "impurity score after eliminating HTML tags: 0\n"
     ]
    }
   ],
   "source": [
    "# let's try first for the text that we have defined before\n",
    "\n",
    "print('before')\n",
    "print(temp_text)\n",
    "\n",
    "print('after')\n",
    "print(clean(temp_text))\n",
    "\n",
    "print('_')\n",
    "print('impurity score before eliminating HTML tags:', impurity(temp_text))\n",
    "print('impurity score after eliminating HTML tags:', impurity(clean(temp_text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>impurity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>737246</th>\n",
       "      <td>7umots</td>\n",
       "      <td>The End is approaching&lt;lb&gt;&lt;lb&gt;&lt;lb&gt;&lt;lb&gt;&lt;lb&gt;&lt;lb&gt;...</td>\n",
       "      <td>0.288783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737556</th>\n",
       "      <td>8lanvf</td>\n",
       "      <td>Darkness creeping in, &lt;lb&gt;&lt;lb&gt;&lt;lb&gt;It’s hard to...</td>\n",
       "      <td>0.194305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737076</th>\n",
       "      <td>7v5kmd</td>\n",
       "      <td>&lt;lb&gt;&lt;lb&gt;&lt;lb&gt;Scars on my body, &lt;lb&gt;&lt;lb&gt;&lt;lb&gt;From...</td>\n",
       "      <td>0.188746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737763</th>\n",
       "      <td>7s6mvn</td>\n",
       "      <td>All right it's been two consecutive nights of ...</td>\n",
       "      <td>0.187552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797544</th>\n",
       "      <td>86r70a</td>\n",
       "      <td>-1-&lt;lb&gt;&lt;lb&gt;Welcome to Republic City&lt;lb&gt;&lt;lb&gt;Reb...</td>\n",
       "      <td>0.184953</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                                               text  impurity\n",
       "737246  7umots  The End is approaching<lb><lb><lb><lb><lb><lb>...  0.288783\n",
       "737556  8lanvf  Darkness creeping in, <lb><lb><lb>It’s hard to...  0.194305\n",
       "737076  7v5kmd  <lb><lb><lb>Scars on my body, <lb><lb><lb>From...  0.188746\n",
       "737763  7s6mvn  All right it's been two consecutive nights of ...  0.187552\n",
       "797544  86r70a  -1-<lb><lb>Welcome to Republic City<lb><lb>Reb...  0.184953"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>impurity_clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>745103</th>\n",
       "      <td>4yui5e</td>\n",
       "      <td>Priscilla&gt;Minerva (Witch of Wrath)&gt;Elsa&gt;Sekhme...</td>\n",
       "      <td>0.055394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328492</th>\n",
       "      <td>5tlvbf</td>\n",
       "      <td>\\&gt;be me, Blake Belladonna &amp;nbsp; \\&gt;be at home ...</td>\n",
       "      <td>0.052251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737176</th>\n",
       "      <td>8fsmz1</td>\n",
       "      <td>**NSFW beacuse you will see... \\(***knifes, ne...</td>\n",
       "      <td>0.046387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737517</th>\n",
       "      <td>7m57i7</td>\n",
       "      <td>(#s) (#s) (#s) (#s) Another thing that's odd i...</td>\n",
       "      <td>0.043478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>659350</th>\n",
       "      <td>5k80tm</td>\n",
       "      <td>Based off of this thread two years ago. We nee...</td>\n",
       "      <td>0.039863</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                                         clean_text  \\\n",
       "745103  4yui5e  Priscilla>Minerva (Witch of Wrath)>Elsa>Sekhme...   \n",
       "328492  5tlvbf  \\>be me, Blake Belladonna &nbsp; \\>be at home ...   \n",
       "737176  8fsmz1  **NSFW beacuse you will see... \\(***knifes, ne...   \n",
       "737517  7m57i7  (#s) (#s) (#s) (#s) Another thing that's odd i...   \n",
       "659350  5k80tm  Based off of this thread two years ago. We nee...   \n",
       "\n",
       "        impurity_clean_text  \n",
       "745103             0.055394  \n",
       "328492             0.052251  \n",
       "737176             0.046387  \n",
       "737517             0.043478  \n",
       "659350             0.039863  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# try the function to our data frame\n",
    "# get the top 5 records with the highest impurity\n",
    "\n",
    "df['clean_text'] = df['text'].map(clean)\n",
    "df['impurity_clean_text'] = df['clean_text'].apply(impurity, min_len=10)\n",
    "\n",
    "# impurity of not clean text\n",
    "display(df[['id', 'text', 'impurity']].sort_values('impurity', ascending=False).head(5))\n",
    "\n",
    "# impurity of clean text\n",
    "display(df[['id', 'clean_text', 'impurity_clean_text']].sort_values('impurity_clean_text', ascending=False).head(5))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Normalization Characters with textacy\n",
    "\n",
    "Normalize all the texts by replacing accents and fancy characters with ASCII equivalents. To work with this, we can use two libraries:\n",
    "\n",
    "- **textacy**   : preprocessin of the text (list of the functions, check p.98)\n",
    "- **spaCy**     : linguistic processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textacy.preprocessing as tprep\n",
    "\n",
    "def normalize(text):\n",
    "    '''normalize text, \n",
    "    the list of the functions are shown below\n",
    "    '''\n",
    "    \n",
    "    text = tprep.normalize.hyphenated_words(text)\n",
    "    text = tprep.normalize.quotation_marks(text)\n",
    "    text = tprep.normalize.unicode(text)\n",
    "    text = tprep.remove.accents(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Voix ambigue d'un cœur qui au zephyr prefere les jattes de kiwis\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's try with an accented text\n",
    "\n",
    "temp_text = \"Voix ambiguë d'un cœur qui au zéphyr préfère les jattes de kiwis\"\n",
    "normalize(temp_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply to our data\n",
    "\n",
    "df['clean_text'] = df['clean_text'].map(normalize)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Pattern-Based Data Masking with textacy\n",
    "\n",
    "The example shown here is to replace URL with string '_URL_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>http://kissanime.ru/Special/Banned/</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://greasyfork.org/en/scripts/4900-kissanime-anti-adblock-blocker</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://translationchicken.com/2016/09/12/rezero-arc-3-interlude-ii-lets-eat-12/</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://www.fanfiction.net/s/12592843/1/Bleach-Towards-a-New-Future-English-Adaptation</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://redd.it/69tevg**</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    freq\n",
       "token                                                   \n",
       "http://kissanime.ru/Special/Banned/                    3\n",
       "https://greasyfork.org/en/scripts/4900-kissanim...     3\n",
       "https://translationchicken.com/2016/09/12/rezer...     3\n",
       "https://www.fanfiction.net/s/12592843/1/Bleach-...     3\n",
       "https://redd.it/69tevg**                               3"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find and show all URL in the text\n",
    "# using textacy.preprocessing.replace.replace_urls\n",
    "\n",
    "import textacy.preprocessing as tprep\n",
    "\n",
    "pp_text_1.count_words(df, 'clean_text', preprocess=tprep.resources.RE_URL.findall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Please open _URL_ before we start the course.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's try with an accented text\n",
    "\n",
    "temp_text = \"Please open http://kissanime.ru/Special/Banned/ before we start the course.\"\n",
    "tprep.replace.urls(temp_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [freq]\n",
       "Index: []"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply to our data\n",
    "\n",
    "df['clean_text'] = df['clean_text'].map(tprep.replace.urls)\n",
    "pp_text_1.count_words(df, 'clean_text', preprocess=tprep.resources.RE_URL.findall)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint - Save Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>raw_text</th>\n",
       "      <th>category</th>\n",
       "      <th>sub_category</th>\n",
       "      <th>sub2_category</th>\n",
       "      <th>text</th>\n",
       "      <th>impurity_clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18000</th>\n",
       "      <td>83z5kr</td>\n",
       "      <td>KissAnime</td>\n",
       "      <td>Randomly getting “Block Ads” warning on site?</td>\n",
       "      <td>I have NEVER had AdBlock, or any other type of...</td>\n",
       "      <td>anime/manga</td>\n",
       "      <td>kiss</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I have NEVER had AdBlock, or any other type of...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18001</th>\n",
       "      <td>6dc2pb</td>\n",
       "      <td>KissAnime</td>\n",
       "      <td>So about this Captcha</td>\n",
       "      <td>I understand that there are post about this al...</td>\n",
       "      <td>anime/manga</td>\n",
       "      <td>kiss</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I understand that there are post about this al...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18002</th>\n",
       "      <td>6j4brs</td>\n",
       "      <td>KissAnime</td>\n",
       "      <td>colorless videos</td>\n",
       "      <td>So, recently, the videos on KissAnime have bee...</td>\n",
       "      <td>anime/manga</td>\n",
       "      <td>kiss</td>\n",
       "      <td>NaN</td>\n",
       "      <td>So, recently, the videos on KissAnime have bee...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18003</th>\n",
       "      <td>6e3fve</td>\n",
       "      <td>KissAnime</td>\n",
       "      <td>Where can i notify the owner/uploaders of kiss...</td>\n",
       "      <td>So i was watching baka to test and i want to t...</td>\n",
       "      <td>anime/manga</td>\n",
       "      <td>kiss</td>\n",
       "      <td>NaN</td>\n",
       "      <td>So i was watching baka to test and i want to t...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18004</th>\n",
       "      <td>5z7dqh</td>\n",
       "      <td>KissAnime</td>\n",
       "      <td>[KODI] I found an alternate kissanime addon wh...</td>\n",
       "      <td>If you log in in the settings, you can see you...</td>\n",
       "      <td>anime/manga</td>\n",
       "      <td>kiss</td>\n",
       "      <td>NaN</td>\n",
       "      <td>If you log in in the settings, you can see you...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id  subreddit                                              title  \\\n",
       "18000  83z5kr  KissAnime      Randomly getting “Block Ads” warning on site?   \n",
       "18001  6dc2pb  KissAnime                              So about this Captcha   \n",
       "18002  6j4brs  KissAnime                                   colorless videos   \n",
       "18003  6e3fve  KissAnime  Where can i notify the owner/uploaders of kiss...   \n",
       "18004  5z7dqh  KissAnime  [KODI] I found an alternate kissanime addon wh...   \n",
       "\n",
       "                                                raw_text     category  \\\n",
       "18000  I have NEVER had AdBlock, or any other type of...  anime/manga   \n",
       "18001  I understand that there are post about this al...  anime/manga   \n",
       "18002  So, recently, the videos on KissAnime have bee...  anime/manga   \n",
       "18003  So i was watching baka to test and i want to t...  anime/manga   \n",
       "18004  If you log in in the settings, you can see you...  anime/manga   \n",
       "\n",
       "      sub_category sub2_category  \\\n",
       "18000         kiss           NaN   \n",
       "18001         kiss           NaN   \n",
       "18002         kiss           NaN   \n",
       "18003         kiss           NaN   \n",
       "18004         kiss           NaN   \n",
       "\n",
       "                                                    text  impurity_clean_text  \n",
       "18000  I have NEVER had AdBlock, or any other type of...                  0.0  \n",
       "18001  I understand that there are post about this al...                  0.0  \n",
       "18002  So, recently, the videos on KissAnime have bee...                  0.0  \n",
       "18003  So i was watching baka to test and i want to t...                  0.0  \n",
       "18004  If you log in in the settings, you can see you...                  0.0  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rename(columns={'text': 'raw_text', 'clean_text': 'text'}, inplace=True)\n",
    "df.drop(columns=['impurity'], inplace=True)\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data frame as the checkpoint\n",
    "\n",
    "db_name = 'reddit-selfposts.db'\n",
    "con = sqlite3.connect(db_name)\n",
    "df.to_sql('anime-posts-cleaned', con, index=False, if_exists='replace')\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linguistic Processing\n",
    "\n",
    "For this one, we will use spaCy. It's a powerful liobrary for linguistic data processing, from Tokenizer, POS tagger, Parser, Named-entity recognizer (NER), etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x7ff4488be820>),\n",
       " ('tagger', <spacy.pipeline.tagger.Tagger at 0x7ff448869100>),\n",
       " ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x7ff448af5f20>),\n",
       " ('attribute_ruler',\n",
       "  <spacy.pipeline.attributeruler.AttributeRuler at 0x7ff44880ef80>),\n",
       " ('lemmatizer',\n",
       "  <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x7ff450842700>),\n",
       " ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x7ff448af56d0>)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_lg')  # setting the language\n",
    "nlp.pipeline  # checking the list of pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lets|go|check|the|footbal|field|today|!|"
     ]
    }
   ],
   "source": [
    "# example tokenization using spacy\n",
    "\n",
    "text = \"Lets go check the footbal field today!\"\n",
    "doc = nlp(text)  \n",
    "\n",
    "for token in doc:\n",
    "    print(token, end='|')\n",
    "    \n",
    "# spacy retains the original text, in the doc as container\n",
    "# if we print doc, it will return the return text\n",
    "# but as we iterate the doc, it will return the preprocessed text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a lot of interesting properties from spacy tokens\n",
    "# this is the function to check the properties of it\n",
    "\n",
    "def display_prop_spacy(doc, include_punct=False):\n",
    "    '''function to show all the properties of the token in form of dataframe\n",
    "    \n",
    "    Parameters:\n",
    "    doc             : the tokenized text from spacy library\n",
    "    include_punct   : True/False\n",
    "    \n",
    "    Return:\n",
    "    dataframe of spacy properties\n",
    "    '''\n",
    "    \n",
    "    rows = []\n",
    "    for i, t in enumerate(doc):\n",
    "        if not t.is_punct or include_punct:\n",
    "            row = {'index': i, 'text': t.text, 'lemma_': t.lemma_,\n",
    "                   'is_stop': t.is_stop, 'is_alpha': t.is_alpha,\n",
    "                   'pos_': t.pos_, 'dep_': t.dep_,\n",
    "                   'ent_type_': t.ent_type_, 'ent_iob': t.ent_iob_}\n",
    "            rows.append(row)\n",
    "    \n",
    "    df = pd.DataFrame(rows).set_index('index')\n",
    "    df.index.name = None\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>lemma_</th>\n",
       "      <th>is_stop</th>\n",
       "      <th>is_alpha</th>\n",
       "      <th>pos_</th>\n",
       "      <th>dep_</th>\n",
       "      <th>ent_type_</th>\n",
       "      <th>ent_iob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lets</td>\n",
       "      <td>let</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>VERB</td>\n",
       "      <td>nsubj</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>go</td>\n",
       "      <td>go</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>VERB</td>\n",
       "      <td>ROOT</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>check</td>\n",
       "      <td>check</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>VERB</td>\n",
       "      <td>advcl</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>DET</td>\n",
       "      <td>det</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>footbal</td>\n",
       "      <td>footbal</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>compound</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>field</td>\n",
       "      <td>field</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>dobj</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>today</td>\n",
       "      <td>today</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>npadvmod</td>\n",
       "      <td>DATE</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      text   lemma_  is_stop  is_alpha  pos_      dep_ ent_type_ ent_iob\n",
       "0     Lets      let    False      True  VERB     nsubj                 O\n",
       "1       go       go     True      True  VERB      ROOT                 O\n",
       "2    check    check    False      True  VERB     advcl                 O\n",
       "3      the      the     True      True   DET       det                 O\n",
       "4  footbal  footbal    False      True   ADJ  compound                 O\n",
       "5    field    field    False      True  NOUN      dobj                 O\n",
       "6    today    today    False      True  NOUN  npadvmod      DATE       B"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's try on our text\n",
    "\n",
    "display_prop_spacy(doc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Customizing Tokenizer    \n",
    "\n",
    "This is to customize the tokenizer available from spaCy. **Be aware!** of modifying the default tokenizer from spaCy as any subtle modifications could break other rules to tokenize the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is only an example on how to modify\n",
    "# I will not use this one for the data I have now\n",
    "# to learn more on how to use this one, read the book PAGE 108\n",
    "\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.util import compile_prefix_regex, compile_infix_regex, compile_suffix_regex\n",
    "\n",
    "def custom_tokenizer_spacy(nlp):\n",
    "    \n",
    "    '''modify the rule here'''\n",
    "    prefixes = [pattern for pattern in nlp.Defaults.prefixes if pattern not in ['-', '_', '#']]\n",
    "    suffixes = [pattern for pattern in nlp.Defaults.suffixes if pattern not in ['_']]\n",
    "    infixes = [pattern for pattern in nlp.Defaults.infixes if not re.search(pattern, 'xx-xx')]\n",
    "    \n",
    "    return Tokenizer(vocab = nlp.vocab,\n",
    "                     rules = nlp.Defaults.tokenizer_exceptions,\n",
    "                     prefix_search  = compile_prefix_regex(prefixes).search,\n",
    "                     suffix_search  = compile_suffix_regex(suffixes).search,\n",
    "                     infix_finditer = compile_infix_regex(infixes).finditer,\n",
    "                     token_match    = nlp.Defaults.token_match)\n",
    "\n",
    "nlp.tokenizer = custom_tokenizer_spacy(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@Pete|:|choose|low-carb|#food|#eat-smart|.|_url_|;-)|😋|👍|"
     ]
    }
   ],
   "source": [
    "temp_text = '@Pete: choose low-carb #food #eat-smart. _url_ ;-) 😋👍'\n",
    "doc = nlp(temp_text)\n",
    "\n",
    "for token in doc:\n",
    "    print(token, end='|')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Customizing Stop Words \n",
    "\n",
    "We also can add or remove stop words from spaCy library.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Stop, words, common, words, language, add, information, text]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# before modifying and customizing the list of stop words\n",
    "\n",
    "temp_text = 'Stop words are the most common words in any language and do not add much information to the text.'\n",
    "doc = nlp(temp_text)\n",
    "\n",
    "non_stop = [token for token in doc if not token.is_stop and not token.is_punct]\n",
    "non_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Stop, words, words, language, add, much, information, text]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now let's change and remove some of these from stop words list\n",
    "# by calling .is_stop = True/False\n",
    "\n",
    "nlp.vocab['much'].is_stop = False\n",
    "nlp.vocab['common'].is_stop = True\n",
    "\n",
    "# re-run again\n",
    "\n",
    "non_stop = [token for token in doc if not token.is_stop and not token.is_punct]\n",
    "non_stop"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Extracting Lemmas Based on Part of Speech (POS)\n",
    "\n",
    "Combining the functions from textacy and spaCy library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before lemmatization:  Joe waited for the train. But, the train was late. In the end, Joe decided to take the bus.\n",
      "after lemmatization: |Joe|wait|for|the|train|.|but|,|the|train|be|late|.|in|the|end|,|Joe|decide|to|take|the|bus|.\n"
     ]
    }
   ],
   "source": [
    "# lemmatization\n",
    "\n",
    "temp_text = 'Joe waited for the train. But, the train was late. In the end, Joe decided to take the bus.'\n",
    "doc = nlp(temp_text)\n",
    "\n",
    "print('before lemmatization: ', temp_text)\n",
    "print('after lemmatization: ', *[t.lemma_ for t in doc], sep='|')\n",
    "\n",
    "# there's an asterisk in print function\n",
    "# the asterisk passes all of the items in list into the print function call as separate arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>lemma_</th>\n",
       "      <th>is_stop</th>\n",
       "      <th>is_alpha</th>\n",
       "      <th>pos_</th>\n",
       "      <th>dep_</th>\n",
       "      <th>ent_type_</th>\n",
       "      <th>ent_iob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Joe</td>\n",
       "      <td>Joe</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>waited</td>\n",
       "      <td>wait</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>VERB</td>\n",
       "      <td>ROOT</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>for</td>\n",
       "      <td>for</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>ADP</td>\n",
       "      <td>prep</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>DET</td>\n",
       "      <td>det</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train</td>\n",
       "      <td>train</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>pobj</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>But</td>\n",
       "      <td>but</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>CCONJ</td>\n",
       "      <td>cc</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>DET</td>\n",
       "      <td>det</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>train</td>\n",
       "      <td>train</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>nsubj</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>was</td>\n",
       "      <td>be</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>AUX</td>\n",
       "      <td>ROOT</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>late</td>\n",
       "      <td>late</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>acomp</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>In</td>\n",
       "      <td>in</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>ADP</td>\n",
       "      <td>prep</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>DET</td>\n",
       "      <td>det</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>end</td>\n",
       "      <td>end</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>pobj</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Joe</td>\n",
       "      <td>Joe</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>decided</td>\n",
       "      <td>decide</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>VERB</td>\n",
       "      <td>ROOT</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>to</td>\n",
       "      <td>to</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>PART</td>\n",
       "      <td>aux</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>take</td>\n",
       "      <td>take</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>VERB</td>\n",
       "      <td>xcomp</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>DET</td>\n",
       "      <td>det</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>bus</td>\n",
       "      <td>bus</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>dobj</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       text  lemma_  is_stop  is_alpha   pos_   dep_ ent_type_ ent_iob\n",
       "0       Joe     Joe    False      True  PROPN  nsubj    PERSON       B\n",
       "1    waited    wait    False      True   VERB   ROOT                 O\n",
       "2       for     for     True      True    ADP   prep                 O\n",
       "3       the     the     True      True    DET    det                 O\n",
       "4     train   train    False      True   NOUN   pobj                 O\n",
       "6       But     but     True      True  CCONJ     cc                 O\n",
       "8       the     the     True      True    DET    det                 O\n",
       "9     train   train    False      True   NOUN  nsubj                 O\n",
       "10      was      be     True      True    AUX   ROOT                 O\n",
       "11     late    late    False      True    ADJ  acomp                 O\n",
       "13       In      in     True      True    ADP   prep                 O\n",
       "14      the     the     True      True    DET    det                 O\n",
       "15      end     end    False      True   NOUN   pobj                 O\n",
       "17      Joe     Joe    False      True  PROPN  nsubj    PERSON       B\n",
       "18  decided  decide    False      True   VERB   ROOT                 O\n",
       "19       to      to     True      True   PART    aux                 O\n",
       "20     take    take     True      True   VERB  xcomp                 O\n",
       "21      the     the     True      True    DET    det                 O\n",
       "22      bus     bus    False      True   NOUN   dobj                 O"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter only the words we're interested to\n",
    "# by showing the pos tag first, then extracting on what we're interested to: the context, which consists of noun and proper noun\n",
    "\n",
    "temp_text = 'Joe waited for the train. But, the train was late. In the end, Joe decided to take the bus.'\n",
    "doc = nlp(temp_text)\n",
    "\n",
    "display_prop_spacy(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Joe, train, train, end, Joe, bus]\n"
     ]
    }
   ],
   "source": [
    "# now, we're going to extract only noun and pronoun\n",
    "\n",
    "nouns = [t for t in doc if t.pos_ in ['PROPN', 'NOUN']]\n",
    "print(nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joe|train|train|late|end|Joe|bus\n"
     ]
    }
   ],
   "source": [
    "# using textacy and apply it to spaCy objects, we can extract and filter based what we need\n",
    "# this is just an example\n",
    "\n",
    "import textacy\n",
    "\n",
    "tokens = textacy.extract.words(doc,\n",
    "                               filter_stops = True,                      # true, filtering stop words\n",
    "                               filter_punct = True,                      # true, filtering punctuations\n",
    "                               filter_nums = True,                       # true, filtering numbers\n",
    "                               include_pos = ['PROPN', 'ADJ', 'NOUN'],   # only include the specified pos\n",
    "                               exclude_pos = None,                       # None, exclude nothing\n",
    "                               min_freq = 1                              # minimal frequency of words\n",
    "                               )\n",
    "\n",
    "print(*[t for t in tokens], sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joe|wait|train|train|late|end|Joe|decide|bus\n"
     ]
    }
   ],
   "source": [
    "# build a wrapper function to extract the lemmatized text\n",
    "# passing kwargs, which will use the same kwargs with what textacy.extract.words is using\n",
    "\n",
    "def extract_lemmas(doc, **kwargs):\n",
    "    return [t.lemma_ for t in textacy.extract.words(doc, **kwargs)]\n",
    "\n",
    "lemmas = extract_lemmas(doc, include_pos=['PROPN', 'VERB', 'ADJ', 'NOUN'])\n",
    "print(*lemmas, sep='|')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Extracting Noun Phrases\n",
    "\n",
    "Using the rule-based matcher from spaCy, and then wrap it as a prarmeter for textacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joe wait|Joe decide\n"
     ]
    }
   ],
   "source": [
    "# tokenize the text first using spaCy\n",
    "# then matching the pattern and pass it to textacy.extract.matches as the patterns parameter\n",
    "\n",
    "import textacy\n",
    "\n",
    "temp_text = 'Joe waited for the train. But, the train was late. In the end, Joe decided to take the bus.'\n",
    "doc = nlp(temp_text)\n",
    "\n",
    "patterns = ['POS:PROPN POS:VERB:+']  # patterns follow the spaCy regex rules, check: https://spacy.io/usage/rule-based-matching\n",
    "spans = textacy.extract.matches.token_matches(doc, patterns=patterns)  # this is different from the one written in the book, updated by textacy\n",
    "print(*[s.lemma_ for s in spans], sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can build a noun-phrases or anything pattern-based match\n",
    "\n",
    "def extract_noun_phrases(doc, pos_pattern, sep='_'):\n",
    "    '''to extract noun-phrases based on pattern of POS\n",
    "    \n",
    "    Parameters:\n",
    "    doc         : preprocessed text using nlp\n",
    "    pos_pattern : list of the pattern for POS\n",
    "    sep         : separator between words\n",
    "    \n",
    "    Return:\n",
    "    clean text that has been preprocessed.\n",
    "    '''\n",
    "    patterns = []\n",
    "    for pos in pos_pattern:\n",
    "        '''\n",
    "        change here if you want to make any changes for the patterns\n",
    "        especially in POS:NOUN:+\n",
    "        based on spaCy regex rules, check: https://spacy.io/usage/rule-based-matching\n",
    "        '''\n",
    "        patterns.append(f'POS:{pos} POS:NOUN:+')        \n",
    "    spans = textacy.extract.matches.token_matches(doc, patterns=patterns)\n",
    "    \n",
    "    return [sep.join([t.lemma_ for t in s]) for s in spans]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good_friend|fancy_adventure|fancy_adventure_game|adventure_game\n"
     ]
    }
   ],
   "source": [
    "# let's try our function now\n",
    "\n",
    "temp_text = 'My best friend Ryan Peters likes fancy adventure games.'\n",
    "doc = nlp(temp_text)\n",
    "print(*extract_noun_phrases(doc, ['ADJ', 'NOUN']), sep='|')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Extracting Named Entities\n",
    "\n",
    "This process is to detect the entities such as the name of the **people, locations, or organizations** in the text.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Lionel Messi, PERSON) (FC Barcelona, ORG) (Blaugrana, PRODUCT) "
     ]
    }
   ],
   "source": [
    "# show text that might shows an entity object\n",
    "# like people, location, or organization\n",
    "\n",
    "temp_text = 'The day Lionel Messi bid a tearful farewell to FC Barcelona was a heartbreaking moment, not only for the Blaugrana, but for the world of football.'\n",
    "doc = nlp(temp_text)\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(f'({ent.text}, {ent.label_})', end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">The day \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Lionel Messi\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " bid a tearful farewell to \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    FC Barcelona\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " was a heartbreaking moment, not only for the \n",
       "<mark class=\"entity\" style=\"background: #bfeeb7; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Blaugrana\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PRODUCT</span>\n",
       "</mark>\n",
       ", but for the world of football.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# other way around, we can use spaCy displacy module to know these entity objects\n",
    "\n",
    "spacy.displacy.render(doc, style='ent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can build a function to extract entities\n",
    "\n",
    "def extract_entities(doc, include_types=None, sep='_'):\n",
    "    ents = textacy.extract.entities(doc,\n",
    "                                    include_types=include_types,\n",
    "                                    exclude_types=None,\n",
    "                                    drop_determiners=True,\n",
    "                                    min_freq=1)\n",
    "    \n",
    "    return [sep.join([t.lemma_ for t in e]) + '/' + e.label_ for e in ents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Lionel_Messi/PERSON', 'FC_Barcelona/ORG']\n"
     ]
    }
   ],
   "source": [
    "print(extract_entities(doc, include_types=['PERSON', 'ORG']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction on the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's build all in one functions from all the \"mini\" functions that we already built before.\n",
    "\n",
    "def extract_nlp(doc):\n",
    "    return{\n",
    "        'lemmas'            : extract_lemmas(doc),\n",
    "        'adjs_verbs'        : extract_lemmas(doc, include_pos = ['ADJ', 'VERB']),\n",
    "        'nouns'             : extract_lemmas(doc, include_pos = ['NOUN', 'PROPN']),\n",
    "        'noun_phrases'      : extract_noun_phrases(doc, ['NOUN']),\n",
    "        'adj_noun_phrases'  : extract_noun_phrases(doc, ['ADJ']),\n",
    "        'entities'          : extract_entities(doc, ['PERSON', 'ORG', 'PRODUCT', 'GPE', 'LOC'])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then we built a dataframe to show and extract all the items from extract_nlp to\n",
    "\n",
    "temp_text = '''\n",
    "At the time, Messi had just won the Copa America with Argentina in 2021 and had told the club that his desire was to stay at Barcelona, with the return of Joan Laporta. \n",
    "However, he had to leave. Messi had a difficult first year at his new club, PSG, but he is already the figurehead he was expected to be in Paris, winning a league title \n",
    "and finally becoming a world champion at international level in Qatar.\n",
    "'''\n",
    "\n",
    "doc = nlp(temp_text)\n",
    "\n",
    "def extract_nlp_df(doc):\n",
    "    i = 0\n",
    "    df_nlp = pd.DataFrame()\n",
    "\n",
    "    for col, values in extract_nlp(doc).items():\n",
    "        df_nlp.loc[i, 'linguistic_att'] = col\n",
    "        df_nlp.loc[i, 'values'] = ', '.join(values)\n",
    "        i += 1\n",
    "    \n",
    "    return df_nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>linguistic_att</th>\n",
       "      <th>values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lemmas</td>\n",
       "      <td>time, Messi, win, Copa, America, Argentina, 20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>adjs_verbs</td>\n",
       "      <td>win, tell, stay, leave, difficult, new, expect...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nouns</td>\n",
       "      <td>time, Messi, Copa, America, Argentina, club, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>noun_phrases</td>\n",
       "      <td>league_title, world_champion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>adj_noun_phrases</td>\n",
       "      <td>first_year, new_club, international_level</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>entities</td>\n",
       "      <td>Messi/PERSON, Argentina/GPE, Barcelona/GPE, Jo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     linguistic_att                                             values\n",
       "0            lemmas  time, Messi, win, Copa, America, Argentina, 20...\n",
       "1        adjs_verbs  win, tell, stay, leave, difficult, new, expect...\n",
       "2             nouns  time, Messi, Copa, America, Argentina, club, d...\n",
       "3      noun_phrases                       league_title, world_champion\n",
       "4  adj_noun_phrases          first_year, new_club, international_level\n",
       "5          entities  Messi/PERSON, Argentina/GPE, Barcelona/GPE, Jo..."
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's try our function to the preprocessed text that we built\n",
    "\n",
    "extract_nlp_df(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All Set, Let's Apply All to Reddit Data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>raw_text</th>\n",
       "      <th>category</th>\n",
       "      <th>sub_category</th>\n",
       "      <th>sub2_category</th>\n",
       "      <th>text</th>\n",
       "      <th>impurity_clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>83z5kr</td>\n",
       "      <td>KissAnime</td>\n",
       "      <td>Randomly getting “Block Ads” warning on site?</td>\n",
       "      <td>I have NEVER had AdBlock, or any other type of...</td>\n",
       "      <td>anime/manga</td>\n",
       "      <td>kiss</td>\n",
       "      <td>None</td>\n",
       "      <td>Randomly getting “Block Ads” warning on site?:...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6dc2pb</td>\n",
       "      <td>KissAnime</td>\n",
       "      <td>So about this Captcha</td>\n",
       "      <td>I understand that there are post about this al...</td>\n",
       "      <td>anime/manga</td>\n",
       "      <td>kiss</td>\n",
       "      <td>None</td>\n",
       "      <td>So about this Captcha:I understand that there ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6j4brs</td>\n",
       "      <td>KissAnime</td>\n",
       "      <td>colorless videos</td>\n",
       "      <td>So, recently, the videos on KissAnime have bee...</td>\n",
       "      <td>anime/manga</td>\n",
       "      <td>kiss</td>\n",
       "      <td>None</td>\n",
       "      <td>colorless videos:So, recently, the videos on K...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6e3fve</td>\n",
       "      <td>KissAnime</td>\n",
       "      <td>Where can i notify the owner/uploaders of kiss...</td>\n",
       "      <td>So i was watching baka to test and i want to t...</td>\n",
       "      <td>anime/manga</td>\n",
       "      <td>kiss</td>\n",
       "      <td>None</td>\n",
       "      <td>Where can i notify the owner/uploaders of kiss...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5z7dqh</td>\n",
       "      <td>KissAnime</td>\n",
       "      <td>[KODI] I found an alternate kissanime addon wh...</td>\n",
       "      <td>If you log in in the settings, you can see you...</td>\n",
       "      <td>anime/manga</td>\n",
       "      <td>kiss</td>\n",
       "      <td>None</td>\n",
       "      <td>[KODI] I found an alternate kissanime addon wh...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  subreddit                                              title  \\\n",
       "0  83z5kr  KissAnime      Randomly getting “Block Ads” warning on site?   \n",
       "1  6dc2pb  KissAnime                              So about this Captcha   \n",
       "2  6j4brs  KissAnime                                   colorless videos   \n",
       "3  6e3fve  KissAnime  Where can i notify the owner/uploaders of kiss...   \n",
       "4  5z7dqh  KissAnime  [KODI] I found an alternate kissanime addon wh...   \n",
       "\n",
       "                                            raw_text     category  \\\n",
       "0  I have NEVER had AdBlock, or any other type of...  anime/manga   \n",
       "1  I understand that there are post about this al...  anime/manga   \n",
       "2  So, recently, the videos on KissAnime have bee...  anime/manga   \n",
       "3  So i was watching baka to test and i want to t...  anime/manga   \n",
       "4  If you log in in the settings, you can see you...  anime/manga   \n",
       "\n",
       "  sub_category sub2_category  \\\n",
       "0         kiss          None   \n",
       "1         kiss          None   \n",
       "2         kiss          None   \n",
       "3         kiss          None   \n",
       "4         kiss          None   \n",
       "\n",
       "                                                text  impurity_clean_text  \n",
       "0  Randomly getting “Block Ads” warning on site?:...                  0.0  \n",
       "1  So about this Captcha:I understand that there ...                  0.0  \n",
       "2  colorless videos:So, recently, the videos on K...                  0.0  \n",
       "3  Where can i notify the owner/uploaders of kiss...                  0.0  \n",
       "4  [KODI] I found an alternate kissanime addon wh...                  0.0  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load our data frame again\n",
    "\n",
    "db_name = 'reddit-selfposts.db'\n",
    "con = sqlite3.connect(db_name)\n",
    "df = pd.read_sql(\"SELECT * FROM 'anime-posts-cleaned'\", con)\n",
    "con.close\n",
    "\n",
    "df['text'] = df['title'] + ':' + df['text'] \n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU found, working on CPU\n"
     ]
    }
   ],
   "source": [
    "# setting up the spaCy engine\n",
    "# spaCy benefits by working and running on GPU\n",
    "\n",
    "if spacy.prefer_gpu():\n",
    "    print('Working on GPU')\n",
    "else:\n",
    "    print('No GPU found, working on CPU')\n",
    "    \n",
    "\n",
    "# then defining spacy as the nlp\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nlp.tokenizer = custom_tokenizer_spacy(nlp)'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can run this one if we want to use the custom tokenizer\n",
    "# uncomment the code below\n",
    "\n",
    "'nlp.tokenizer = custom_tokenizer_spacy(nlp)'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we're going to preprocess the data we have. When processing with a large dataset with spaCy, it's advisable to **perform the it in a batch processing** for a significant performance gain. Before we define the batch size and apply it to our dataframe, let's \n",
    "\n",
    "1. Define everything we want to extract from the extract_nlp function\n",
    "2. Create an empty dataframe, we will inject the data/values to the data frame later\n",
    "3. Apply all to our data frame with batch preprocesing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the extract_nlp function\n",
    "# we can adjust what we want to extract from the text\n",
    "\n",
    "def extract_nlp(doc):\n",
    "    '''\n",
    "    we can define and adjust what we want to extract from here\n",
    "    '''\n",
    "    \n",
    "    return{\n",
    "        'lemmas'            : extract_lemmas(doc, \n",
    "                                             exclude_pos = ['PART', 'PUNCT', 'DET', 'PRON', 'SYM', 'SPACE'], \n",
    "                                             filter_stops = False),\n",
    "        'adjs_verbs'        : extract_lemmas(doc, include_pos = ['ADJ', 'VERB']),\n",
    "        'nouns'             : extract_lemmas(doc, include_pos = ['NOUN', 'PROPN']),\n",
    "        'noun_phrases'      : extract_noun_phrases(doc, ['NOUN']),\n",
    "        'adj_noun_phrases'  : extract_noun_phrases(doc, ['ADJ']),\n",
    "        'entities'          : extract_entities(doc, ['PERSON', 'ORG', 'GPE', 'LOC'])\n",
    "    }\n",
    "\n",
    "# this is to build the dataframe\n",
    "# we have to define the column first\n",
    "\n",
    "nlp_columns = list(extract_nlp(nlp.make_doc('')).keys())\n",
    "\n",
    "for col in nlp_columns:\n",
    "    df[col] = None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete linguistic processing takes a lot of time. Despite the fact that spaCy is extremely quick in comparison to other libraries, **it is the tagging, parsing, and named-entity recognition that are expensive**. In order to save more than 60% of the runtime, you should disable the parser and name-entity recognition if you don't require them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/26 [00:00<?, ?it/s]/var/folders/3z/jblqnqjn04ldn_gk07kslfp00000gn/T/ipykernel_4538/1715486843.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_tqdm_test[col].iloc[i+j] = values\n",
      "100%|██████████| 26/26 [00:27<00:00,  1.07s/it]\n"
     ]
    }
   ],
   "source": [
    "# we can integrate tqdm if we want to have the progress bar to know the progress\n",
    "# this is only a testing\n",
    "\n",
    "df_tqdm_test = df.sample(frac=0.05, replace=True, random_state=1)\n",
    "batch_size = 50\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "for i in tqdm(range(0, len(df_tqdm_test), batch_size)):\n",
    "    docs = nlp.pipe(df_tqdm_test['text'][i:i+batch_size])\n",
    "    \n",
    "    for j, doc in enumerate(docs):\n",
    "        for col, values in extract_nlp(doc).items():\n",
    "            df_tqdm_test[col].iloc[i+j] = values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3z/jblqnqjn04ldn_gk07kslfp00000gn/T/ipykernel_4538/2937344372.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[col].iloc[i+j] = values\n"
     ]
    }
   ],
   "source": [
    "# beware when running this, it will take a few minute to finish\n",
    "\n",
    "batch_size = 50\n",
    "\n",
    "for i in range(0, len(df), batch_size):\n",
    "    docs = nlp.pipe(df['text'][i:i+batch_size])\n",
    "    \n",
    "    for j, doc in enumerate(docs):\n",
    "        for col, values in extract_nlp(doc).items():\n",
    "            df[col].iloc[i+j] = values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>raw_text</th>\n",
       "      <th>category</th>\n",
       "      <th>sub_category</th>\n",
       "      <th>sub2_category</th>\n",
       "      <th>text</th>\n",
       "      <th>impurity_clean_text</th>\n",
       "      <th>lemmas</th>\n",
       "      <th>adjs_verbs</th>\n",
       "      <th>nouns</th>\n",
       "      <th>noun_phrases</th>\n",
       "      <th>adj_noun_phrases</th>\n",
       "      <th>entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>83z5kr</td>\n",
       "      <td>KissAnime</td>\n",
       "      <td>Randomly getting “Block Ads” warning on site?</td>\n",
       "      <td>I have NEVER had AdBlock, or any other type of...</td>\n",
       "      <td>anime/manga</td>\n",
       "      <td>kiss</td>\n",
       "      <td>None</td>\n",
       "      <td>Randomly getting “Block Ads” warning on site?:...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[randomly, get, Block, Ads, warning, on, site?...</td>\n",
       "      <td>[get, instal, watch, plentyy, start, lock, blo...</td>\n",
       "      <td>[Block, Ads, warning, site?:I, AdBlock, type, ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[other_type, other_day]</td>\n",
       "      <td>[never/ORG, AdBlock/ORG]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6dc2pb</td>\n",
       "      <td>KissAnime</td>\n",
       "      <td>So about this Captcha</td>\n",
       "      <td>I understand that there are post about this al...</td>\n",
       "      <td>anime/manga</td>\n",
       "      <td>kiss</td>\n",
       "      <td>None</td>\n",
       "      <td>So about this Captcha:I understand that there ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[so, about, Captcha, understand, that, be, pos...</td>\n",
       "      <td>[understand, holy, ridiculous, need, stupid, c...</td>\n",
       "      <td>[Captcha, post, shit, shit, episode, show, tim...</td>\n",
       "      <td>[website_thought]</td>\n",
       "      <td>[holy_shit, stupid_shit, long_show, single_tim...</td>\n",
       "      <td>[Captcha/ORG, KissAnime/ORG, thought/PERSON]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6j4brs</td>\n",
       "      <td>KissAnime</td>\n",
       "      <td>colorless videos</td>\n",
       "      <td>So, recently, the videos on KissAnime have bee...</td>\n",
       "      <td>anime/manga</td>\n",
       "      <td>kiss</td>\n",
       "      <td>None</td>\n",
       "      <td>colorless videos:So, recently, the videos on K...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[colorless, video, so, recently, video, on, Ki...</td>\n",
       "      <td>[colorless, gray, know, mean, multiple, scryed...</td>\n",
       "      <td>[colorless, video, video, KissAnime, color, an...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[same_problem]</td>\n",
       "      <td>[KissAnime/ORG, scryed/GPE, Berserk/GPE]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6e3fve</td>\n",
       "      <td>KissAnime</td>\n",
       "      <td>Where can i notify the owner/uploaders of kiss...</td>\n",
       "      <td>So i was watching baka to test and i want to t...</td>\n",
       "      <td>anime/manga</td>\n",
       "      <td>kiss</td>\n",
       "      <td>None</td>\n",
       "      <td>Where can i notify the owner/uploaders of kiss...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[where, can, notify, owner, uploader, of, kiss...</td>\n",
       "      <td>[notify, have, shitty, watch, test, want, tell...</td>\n",
       "      <td>[owner, uploader, kissanime, anime, sub?:So, b...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[un/ORG, un/ORG]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5z7dqh</td>\n",
       "      <td>KissAnime</td>\n",
       "      <td>[KODI] I found an alternate kissanime addon wh...</td>\n",
       "      <td>If you log in in the settings, you can see you...</td>\n",
       "      <td>anime/manga</td>\n",
       "      <td>kiss</td>\n",
       "      <td>None</td>\n",
       "      <td>[KODI] I found an alternate kissanime addon wh...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[kodi, find, alternate, kissanime, addon, wher...</td>\n",
       "      <td>[find, alternate, work, break, log, entire, pl...</td>\n",
       "      <td>[kissanime, addon, poster, fanart, metadata, s...</td>\n",
       "      <td>[bookmark_folder, working_order]</td>\n",
       "      <td>[alternate_kissanime, entire_bookmark, entire_...</td>\n",
       "      <td>[metadata/PERSON, trakt/GPE, addon/GPE]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  subreddit                                              title  \\\n",
       "0  83z5kr  KissAnime      Randomly getting “Block Ads” warning on site?   \n",
       "1  6dc2pb  KissAnime                              So about this Captcha   \n",
       "2  6j4brs  KissAnime                                   colorless videos   \n",
       "3  6e3fve  KissAnime  Where can i notify the owner/uploaders of kiss...   \n",
       "4  5z7dqh  KissAnime  [KODI] I found an alternate kissanime addon wh...   \n",
       "\n",
       "                                            raw_text     category  \\\n",
       "0  I have NEVER had AdBlock, or any other type of...  anime/manga   \n",
       "1  I understand that there are post about this al...  anime/manga   \n",
       "2  So, recently, the videos on KissAnime have bee...  anime/manga   \n",
       "3  So i was watching baka to test and i want to t...  anime/manga   \n",
       "4  If you log in in the settings, you can see you...  anime/manga   \n",
       "\n",
       "  sub_category sub2_category  \\\n",
       "0         kiss          None   \n",
       "1         kiss          None   \n",
       "2         kiss          None   \n",
       "3         kiss          None   \n",
       "4         kiss          None   \n",
       "\n",
       "                                                text  impurity_clean_text  \\\n",
       "0  Randomly getting “Block Ads” warning on site?:...                  0.0   \n",
       "1  So about this Captcha:I understand that there ...                  0.0   \n",
       "2  colorless videos:So, recently, the videos on K...                  0.0   \n",
       "3  Where can i notify the owner/uploaders of kiss...                  0.0   \n",
       "4  [KODI] I found an alternate kissanime addon wh...                  0.0   \n",
       "\n",
       "                                              lemmas  \\\n",
       "0  [randomly, get, Block, Ads, warning, on, site?...   \n",
       "1  [so, about, Captcha, understand, that, be, pos...   \n",
       "2  [colorless, video, so, recently, video, on, Ki...   \n",
       "3  [where, can, notify, owner, uploader, of, kiss...   \n",
       "4  [kodi, find, alternate, kissanime, addon, wher...   \n",
       "\n",
       "                                          adjs_verbs  \\\n",
       "0  [get, instal, watch, plentyy, start, lock, blo...   \n",
       "1  [understand, holy, ridiculous, need, stupid, c...   \n",
       "2  [colorless, gray, know, mean, multiple, scryed...   \n",
       "3  [notify, have, shitty, watch, test, want, tell...   \n",
       "4  [find, alternate, work, break, log, entire, pl...   \n",
       "\n",
       "                                               nouns  \\\n",
       "0  [Block, Ads, warning, site?:I, AdBlock, type, ...   \n",
       "1  [Captcha, post, shit, shit, episode, show, tim...   \n",
       "2  [colorless, video, video, KissAnime, color, an...   \n",
       "3  [owner, uploader, kissanime, anime, sub?:So, b...   \n",
       "4  [kissanime, addon, poster, fanart, metadata, s...   \n",
       "\n",
       "                       noun_phrases  \\\n",
       "0                                []   \n",
       "1                 [website_thought]   \n",
       "2                                []   \n",
       "3                                []   \n",
       "4  [bookmark_folder, working_order]   \n",
       "\n",
       "                                    adj_noun_phrases  \\\n",
       "0                            [other_type, other_day]   \n",
       "1  [holy_shit, stupid_shit, long_show, single_tim...   \n",
       "2                                     [same_problem]   \n",
       "3                                                 []   \n",
       "4  [alternate_kissanime, entire_bookmark, entire_...   \n",
       "\n",
       "                                       entities  \n",
       "0                      [never/ORG, AdBlock/ORG]  \n",
       "1  [Captcha/ORG, KissAnime/ORG, thought/PERSON]  \n",
       "2      [KissAnime/ORG, scryed/GPE, Berserk/GPE]  \n",
       "3                              [un/ORG, un/ORG]  \n",
       "4       [metadata/PERSON, trakt/GPE, addon/GPE]  "
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's see if we want to calculate the highest number of the noun phrases we have\n",
    "\n",
    "import fun_preprocessing_text as pp_text_1\n",
    "\n",
    "df_temp = pp_text_1.count_words(df, 'noun_phrases').head(12).reset_index()\n",
    "\n",
    "sns.barplot(x='freq', y='token', \n",
    "            data=df_temp[~df_temp['token'].isin(['url__', '*_*', '__url'])], \n",
    "            label=\"Total\", color=\"b\")\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>raw_text</th>\n",
       "      <th>category</th>\n",
       "      <th>sub_category</th>\n",
       "      <th>sub2_category</th>\n",
       "      <th>text</th>\n",
       "      <th>impurity_clean_text</th>\n",
       "      <th>lemmas</th>\n",
       "      <th>adjs_verbs</th>\n",
       "      <th>nouns</th>\n",
       "      <th>noun_phrases</th>\n",
       "      <th>adj_noun_phrases</th>\n",
       "      <th>entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>83z5kr</td>\n",
       "      <td>KissAnime</td>\n",
       "      <td>Randomly getting “Block Ads” warning on site?</td>\n",
       "      <td>I have NEVER had AdBlock, or any other type of...</td>\n",
       "      <td>anime/manga</td>\n",
       "      <td>kiss</td>\n",
       "      <td>None</td>\n",
       "      <td>Randomly getting “Block Ads” warning on site?:...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>randomly get Block Ads warning on site?:I have...</td>\n",
       "      <td>get instal watch plentyy start lock block fix</td>\n",
       "      <td>Block Ads warning site?:I AdBlock type blocker...</td>\n",
       "      <td></td>\n",
       "      <td>other_type other_day</td>\n",
       "      <td>never/ORG AdBlock/ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6dc2pb</td>\n",
       "      <td>KissAnime</td>\n",
       "      <td>So about this Captcha</td>\n",
       "      <td>I understand that there are post about this al...</td>\n",
       "      <td>anime/manga</td>\n",
       "      <td>kiss</td>\n",
       "      <td>None</td>\n",
       "      <td>So about this Captcha:I understand that there ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>so about Captcha understand that be post about...</td>\n",
       "      <td>understand holy ridiculous need stupid change ...</td>\n",
       "      <td>Captcha post shit shit episode show time episo...</td>\n",
       "      <td>website_thought</td>\n",
       "      <td>holy_shit stupid_shit long_show single_time ki...</td>\n",
       "      <td>Captcha/ORG KissAnime/ORG thought/PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6j4brs</td>\n",
       "      <td>KissAnime</td>\n",
       "      <td>colorless videos</td>\n",
       "      <td>So, recently, the videos on KissAnime have bee...</td>\n",
       "      <td>anime/manga</td>\n",
       "      <td>kiss</td>\n",
       "      <td>None</td>\n",
       "      <td>colorless videos:So, recently, the videos on K...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>colorless video so recently video on KissAnime...</td>\n",
       "      <td>colorless gray know mean multiple scryed ask g...</td>\n",
       "      <td>colorless video video KissAnime color anime Be...</td>\n",
       "      <td></td>\n",
       "      <td>same_problem</td>\n",
       "      <td>KissAnime/ORG scryed/GPE Berserk/GPE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6e3fve</td>\n",
       "      <td>KissAnime</td>\n",
       "      <td>Where can i notify the owner/uploaders of kiss...</td>\n",
       "      <td>So i was watching baka to test and i want to t...</td>\n",
       "      <td>anime/manga</td>\n",
       "      <td>kiss</td>\n",
       "      <td>None</td>\n",
       "      <td>Where can i notify the owner/uploaders of kiss...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>where can notify owner uploader of kissanime a...</td>\n",
       "      <td>notify have shitty watch test want tell nee up...</td>\n",
       "      <td>owner uploader kissanime anime sub?:So baka so...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>un/ORG un/ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5z7dqh</td>\n",
       "      <td>KissAnime</td>\n",
       "      <td>[KODI] I found an alternate kissanime addon wh...</td>\n",
       "      <td>If you log in in the settings, you can see you...</td>\n",
       "      <td>anime/manga</td>\n",
       "      <td>kiss</td>\n",
       "      <td>None</td>\n",
       "      <td>[KODI] I found an alternate kissanime addon wh...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>kodi find alternate kissanime addon where post...</td>\n",
       "      <td>find alternate work break log entire play let ...</td>\n",
       "      <td>kissanime addon poster fanart metadata setting...</td>\n",
       "      <td>bookmark_folder working_order</td>\n",
       "      <td>alternate_kissanime entire_bookmark entire_boo...</td>\n",
       "      <td>metadata/PERSON trakt/GPE addon/GPE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25995</th>\n",
       "      <td>6x7d95</td>\n",
       "      <td>HunterXHunter</td>\n",
       "      <td>Hatsu which confuses you the most</td>\n",
       "      <td>Hunter x hunter has a ton of different hatsus,...</td>\n",
       "      <td>anime/manga</td>\n",
       "      <td>hunter x hunter</td>\n",
       "      <td>None</td>\n",
       "      <td>Hatsu which confuses you the most:Hunter x hun...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Hatsu confuse most Hunter hunter have ton of d...</td>\n",
       "      <td>confuse different simple complex find confusin...</td>\n",
       "      <td>Hatsu Hunter hunter ton hatsus lot hatsus rest...</td>\n",
       "      <td></td>\n",
       "      <td>different_hatsus</td>\n",
       "      <td>Hatsu/GPE Hunter/PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25996</th>\n",
       "      <td>7iip96</td>\n",
       "      <td>HunterXHunter</td>\n",
       "      <td>Scanner in NGL not reacting to killua's needle?</td>\n",
       "      <td>After the NGL arc, killua finds out about the ...</td>\n",
       "      <td>anime/manga</td>\n",
       "      <td>hunter x hunter</td>\n",
       "      <td>None</td>\n",
       "      <td>Scanner in NGL not reacting to killua's needle...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>scanner in NGL react to killua needle?:after N...</td>\n",
       "      <td>react find get scan hide happen react bad</td>\n",
       "      <td>scanner NGL killua needle?:after NGL arc killu...</td>\n",
       "      <td>needle_right?(sorry</td>\n",
       "      <td></td>\n",
       "      <td>NGL/GPE killua/PERSON NGL/ORG killua/PERSON NG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25997</th>\n",
       "      <td>7e0xds</td>\n",
       "      <td>HunterXHunter</td>\n",
       "      <td>I'm a little late to the party, but Ultra Inst...</td>\n",
       "      <td>So after googling Ultra Instinct God Speed and...</td>\n",
       "      <td>anime/manga</td>\n",
       "      <td>hunter x hunter</td>\n",
       "      <td>None</td>\n",
       "      <td>I'm a little late to the party, but Ultra Inst...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>be little late to party but Ultra Instinct be ...</td>\n",
       "      <td>little late Speed.:So google find compare sage...</td>\n",
       "      <td>party Ultra Instinct God Ultra Instinct God Sp...</td>\n",
       "      <td>hxh_reddit</td>\n",
       "      <td>blatant_rip unique_ability</td>\n",
       "      <td>Ultra_Instinct/PERSON Ultra_Instinct/PERSON Na...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25998</th>\n",
       "      <td>7l9shb</td>\n",
       "      <td>HunterXHunter</td>\n",
       "      <td>Something I've just realized about Chimera Ant...</td>\n",
       "      <td>So I've been thinking about the end of Chimera...</td>\n",
       "      <td>anime/manga</td>\n",
       "      <td>hunter x hunter</td>\n",
       "      <td>None</td>\n",
       "      <td>Something I've just realized about Chimera Ant...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>'ve just realize about Chimera Ants Ending Tra...</td>\n",
       "      <td>realize think absolute stroke meruem know diso...</td>\n",
       "      <td>Chimera Ants Ending Tragedy SPOILER]:So end Ch...</td>\n",
       "      <td>palace_invasion</td>\n",
       "      <td>absolute_masterpiece meruem_side true_goal bri...</td>\n",
       "      <td>Chimera_Ants_Ending_Tragedy/ORG Chimera_Ants/O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25999</th>\n",
       "      <td>4o8fta</td>\n",
       "      <td>HunterXHunter</td>\n",
       "      <td>So I decided to go ahead and re-watch DBZ afte...</td>\n",
       "      <td>And as I'm watching it, I'm thinking in my hea...</td>\n",
       "      <td>anime/manga</td>\n",
       "      <td>hunter x hunter</td>\n",
       "      <td>None</td>\n",
       "      <td>So I decided to go ahead and re-watch DBZ afte...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>so decide go ahead and re watch DBZ after fini...</td>\n",
       "      <td>decide watch finish watch think breeze sure pa...</td>\n",
       "      <td>DBZ HxH :And head time hxh character Freiza An...</td>\n",
       "      <td>hxh_character hxh_character</td>\n",
       "      <td>whole_time</td>\n",
       "      <td>DBZ/ORG HxH/ORG Jero/PERSON Meruem/PERSON hxh/ORG</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26000 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id      subreddit  \\\n",
       "0      83z5kr      KissAnime   \n",
       "1      6dc2pb      KissAnime   \n",
       "2      6j4brs      KissAnime   \n",
       "3      6e3fve      KissAnime   \n",
       "4      5z7dqh      KissAnime   \n",
       "...       ...            ...   \n",
       "25995  6x7d95  HunterXHunter   \n",
       "25996  7iip96  HunterXHunter   \n",
       "25997  7e0xds  HunterXHunter   \n",
       "25998  7l9shb  HunterXHunter   \n",
       "25999  4o8fta  HunterXHunter   \n",
       "\n",
       "                                                   title  \\\n",
       "0          Randomly getting “Block Ads” warning on site?   \n",
       "1                                  So about this Captcha   \n",
       "2                                       colorless videos   \n",
       "3      Where can i notify the owner/uploaders of kiss...   \n",
       "4      [KODI] I found an alternate kissanime addon wh...   \n",
       "...                                                  ...   \n",
       "25995                  Hatsu which confuses you the most   \n",
       "25996    Scanner in NGL not reacting to killua's needle?   \n",
       "25997  I'm a little late to the party, but Ultra Inst...   \n",
       "25998  Something I've just realized about Chimera Ant...   \n",
       "25999  So I decided to go ahead and re-watch DBZ afte...   \n",
       "\n",
       "                                                raw_text     category  \\\n",
       "0      I have NEVER had AdBlock, or any other type of...  anime/manga   \n",
       "1      I understand that there are post about this al...  anime/manga   \n",
       "2      So, recently, the videos on KissAnime have bee...  anime/manga   \n",
       "3      So i was watching baka to test and i want to t...  anime/manga   \n",
       "4      If you log in in the settings, you can see you...  anime/manga   \n",
       "...                                                  ...          ...   \n",
       "25995  Hunter x hunter has a ton of different hatsus,...  anime/manga   \n",
       "25996  After the NGL arc, killua finds out about the ...  anime/manga   \n",
       "25997  So after googling Ultra Instinct God Speed and...  anime/manga   \n",
       "25998  So I've been thinking about the end of Chimera...  anime/manga   \n",
       "25999  And as I'm watching it, I'm thinking in my hea...  anime/manga   \n",
       "\n",
       "          sub_category sub2_category  \\\n",
       "0                 kiss          None   \n",
       "1                 kiss          None   \n",
       "2                 kiss          None   \n",
       "3                 kiss          None   \n",
       "4                 kiss          None   \n",
       "...                ...           ...   \n",
       "25995  hunter x hunter          None   \n",
       "25996  hunter x hunter          None   \n",
       "25997  hunter x hunter          None   \n",
       "25998  hunter x hunter          None   \n",
       "25999  hunter x hunter          None   \n",
       "\n",
       "                                                    text  impurity_clean_text  \\\n",
       "0      Randomly getting “Block Ads” warning on site?:...                  0.0   \n",
       "1      So about this Captcha:I understand that there ...                  0.0   \n",
       "2      colorless videos:So, recently, the videos on K...                  0.0   \n",
       "3      Where can i notify the owner/uploaders of kiss...                  0.0   \n",
       "4      [KODI] I found an alternate kissanime addon wh...                  0.0   \n",
       "...                                                  ...                  ...   \n",
       "25995  Hatsu which confuses you the most:Hunter x hun...                  0.0   \n",
       "25996  Scanner in NGL not reacting to killua's needle...                  0.0   \n",
       "25997  I'm a little late to the party, but Ultra Inst...                  0.0   \n",
       "25998  Something I've just realized about Chimera Ant...                  0.0   \n",
       "25999  So I decided to go ahead and re-watch DBZ afte...                  0.0   \n",
       "\n",
       "                                                  lemmas  \\\n",
       "0      randomly get Block Ads warning on site?:I have...   \n",
       "1      so about Captcha understand that be post about...   \n",
       "2      colorless video so recently video on KissAnime...   \n",
       "3      where can notify owner uploader of kissanime a...   \n",
       "4      kodi find alternate kissanime addon where post...   \n",
       "...                                                  ...   \n",
       "25995  Hatsu confuse most Hunter hunter have ton of d...   \n",
       "25996  scanner in NGL react to killua needle?:after N...   \n",
       "25997  be little late to party but Ultra Instinct be ...   \n",
       "25998  've just realize about Chimera Ants Ending Tra...   \n",
       "25999  so decide go ahead and re watch DBZ after fini...   \n",
       "\n",
       "                                              adjs_verbs  \\\n",
       "0          get instal watch plentyy start lock block fix   \n",
       "1      understand holy ridiculous need stupid change ...   \n",
       "2      colorless gray know mean multiple scryed ask g...   \n",
       "3      notify have shitty watch test want tell nee up...   \n",
       "4      find alternate work break log entire play let ...   \n",
       "...                                                  ...   \n",
       "25995  confuse different simple complex find confusin...   \n",
       "25996          react find get scan hide happen react bad   \n",
       "25997  little late Speed.:So google find compare sage...   \n",
       "25998  realize think absolute stroke meruem know diso...   \n",
       "25999  decide watch finish watch think breeze sure pa...   \n",
       "\n",
       "                                                   nouns  \\\n",
       "0      Block Ads warning site?:I AdBlock type blocker...   \n",
       "1      Captcha post shit shit episode show time episo...   \n",
       "2      colorless video video KissAnime color anime Be...   \n",
       "3      owner uploader kissanime anime sub?:So baka so...   \n",
       "4      kissanime addon poster fanart metadata setting...   \n",
       "...                                                  ...   \n",
       "25995  Hatsu Hunter hunter ton hatsus lot hatsus rest...   \n",
       "25996  scanner NGL killua needle?:after NGL arc killu...   \n",
       "25997  party Ultra Instinct God Ultra Instinct God Sp...   \n",
       "25998  Chimera Ants Ending Tragedy SPOILER]:So end Ch...   \n",
       "25999  DBZ HxH :And head time hxh character Freiza An...   \n",
       "\n",
       "                        noun_phrases  \\\n",
       "0                                      \n",
       "1                    website_thought   \n",
       "2                                      \n",
       "3                                      \n",
       "4      bookmark_folder working_order   \n",
       "...                              ...   \n",
       "25995                                  \n",
       "25996            needle_right?(sorry   \n",
       "25997                     hxh_reddit   \n",
       "25998                palace_invasion   \n",
       "25999    hxh_character hxh_character   \n",
       "\n",
       "                                        adj_noun_phrases  \\\n",
       "0                                   other_type other_day   \n",
       "1      holy_shit stupid_shit long_show single_time ki...   \n",
       "2                                           same_problem   \n",
       "3                                                          \n",
       "4      alternate_kissanime entire_bookmark entire_boo...   \n",
       "...                                                  ...   \n",
       "25995                                   different_hatsus   \n",
       "25996                                                      \n",
       "25997                         blatant_rip unique_ability   \n",
       "25998  absolute_masterpiece meruem_side true_goal bri...   \n",
       "25999                                         whole_time   \n",
       "\n",
       "                                                entities  \n",
       "0                                  never/ORG AdBlock/ORG  \n",
       "1               Captcha/ORG KissAnime/ORG thought/PERSON  \n",
       "2                   KissAnime/ORG scryed/GPE Berserk/GPE  \n",
       "3                                          un/ORG un/ORG  \n",
       "4                    metadata/PERSON trakt/GPE addon/GPE  \n",
       "...                                                  ...  \n",
       "25995                            Hatsu/GPE Hunter/PERSON  \n",
       "25996  NGL/GPE killua/PERSON NGL/ORG killua/PERSON NG...  \n",
       "25997  Ultra_Instinct/PERSON Ultra_Instinct/PERSON Na...  \n",
       "25998  Chimera_Ants_Ending_Tragedy/ORG Chimera_Ants/O...  \n",
       "25999  DBZ/ORG HxH/ORG Jero/PERSON Meruem/PERSON hxh/ORG  \n",
       "\n",
       "[26000 rows x 15 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can save the dataframe as well to our already-built database\n",
    "\n",
    "db_name = 'reddit-selfposts.db'\n",
    "con = sqlite3.connect(db_name)\n",
    "df.to_sql('anime-posts_nlp', con, index=False, if_exists='replace')\n",
    "con.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0fd9779e8d8f386467a1f1d1102cd9c30d79c45b15374f3d42e7180eb33a0483"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
