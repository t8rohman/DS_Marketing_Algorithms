{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering and Syntactic Similarity: ABC News Dataset\n",
    "\n",
    "Tutorial on how to do a feature engineering and syntatic similarity on a corpus. Functions referred to Blueprints for Text Analytics by Albrecht et al. (2021) with several adjustments to make it more clear. For the ready-to-use functions, please refer to file **fun_syntatic_similarity.py.**\n",
    "\n",
    "In Feature Engineering, it will show you:\n",
    "- How to vectorizing data\n",
    "- Calculate similarities with cosine similarity\n",
    "- Improving time efficiency\n",
    "- Improving the feature itself\n",
    "\n",
    "After that, for Syntatic Similarity, it's about the application. It will explain you:\n",
    "- Finding similar document from the document that you made up (like search engine)\n",
    "- Finding the most 2 similar documents from a corpus\n",
    "- Finding similar words from documents in corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "\n",
    "# libraries for the machine learning models\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  \n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic of Feature Engineering\n",
    "\n",
    "1. Vectorizing Data\n",
    "2. Calculating Similarities"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Own Vectorizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Create the dummy data by creating sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = ['It was the best of times',\n",
    "             'it was the worst of times',\n",
    "             'it was the age of wisdom',\n",
    "             'it was the age of foolishness']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Tokenizing text by splitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['It', 'was', 'the', 'best', 'of', 'times']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# basic of splitting the sentence\n",
    "# the iterating each of the items in the list\n",
    "\n",
    "sentences[0].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['It', 'was', 'the', 'best', 'of', 'times'],\n",
       " ['it', 'was', 'the', 'worst', 'of', 'times'],\n",
       " ['it', 'was', 'the', 'age', 'of', 'wisdom'],\n",
       " ['it', 'was', 'the', 'age', 'of', 'foolishness']]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'It',\n",
       " 'age',\n",
       " 'best',\n",
       " 'foolishness',\n",
       " 'it',\n",
       " 'of',\n",
       " 'the',\n",
       " 'times',\n",
       " 'was',\n",
       " 'wisdom',\n",
       " 'worst'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>it</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>best</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>was</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>of</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>times</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>worst</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>age</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>wisdom</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>It</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>foolishness</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0   1\n",
       "0            it   0\n",
       "1          best   1\n",
       "2           was   2\n",
       "3           the   3\n",
       "4            of   4\n",
       "5         times   5\n",
       "6         worst   6\n",
       "7           age   7\n",
       "8        wisdom   8\n",
       "9            It   9\n",
       "10  foolishness  10"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compile to a list of tokenized text\n",
    "tokenized_sentences = [[token for token in sentence.split()] for sentence in sentences]\n",
    "display(tokenized_sentences)\n",
    "\n",
    "# then convert the list to a set\n",
    "vocabulary = set([word for sentence in tokenized_sentences for word in sentence])\n",
    "display(vocabulary)\n",
    "\n",
    "# convert it to pandas\n",
    "# to make the dictionary dataframe and it's word order\n",
    "pd.DataFrame([word, i] for i, word in enumerate(vocabulary))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Vectorizing Documents and Building Document Term Matrix\n",
    "\n",
    "Comparing and calculating the sentences we have to the vocabulary data frame.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0]: It was the best of times\n",
      "[1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0]: it was the worst of times\n",
      "[1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0]: it was the age of wisdom\n",
      "[1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1]: it was the age of foolishness\n"
     ]
    }
   ],
   "source": [
    "# create function to one-hot encode the tokenized sentence\n",
    "def onehot_encode(tokenized_sentence):\n",
    "    return[1 if word in tokenized_sentence else 0 for word in vocabulary]\n",
    "\n",
    "# one-hot encode every sentence in the tokenized sentences list\n",
    "# using the for in iteration\n",
    "onehot = [onehot_encode(tokenized_sentence) for tokenized_sentence in tokenized_sentences]\n",
    "\n",
    "# print the one hot encoded sentence result\n",
    "for (sentence, oh) in zip (sentences, onehot):\n",
    "    print(f'{oh}' + ': ' + f'{sentence}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>it</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>best</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>was</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>of</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>times</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>worst</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>age</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>wisdom</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>It</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>foolishness</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0   1\n",
       "0            it   0\n",
       "1          best   1\n",
       "2           was   2\n",
       "3           the   3\n",
       "4            of   4\n",
       "5         times   5\n",
       "6         worst   6\n",
       "7           age   7\n",
       "8        wisdom   8\n",
       "9            It   9\n",
       "10  foolishness  10"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([word, i] for i, word in enumerate(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# testing the data using out-of-vocabulary documents\n",
    "\n",
    "print(onehot_encode('the age of wisdom is the best of times'.split()))  # with known vocabulary\n",
    "\n",
    "# without known vocabulary\n",
    "# BEWARE! that it actually can detect part of the letter from vocabulary, like \"of\" from pr(OF)essional\n",
    "print(onehot_encode('Lionel Andr√©s Messi, also known as Leo Messi, is an Argentine professional footballer who plays as a forward for Ligue 1')) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A document-term matrix** is a mathematical matrix that describes the frequency of terms that occur in a collection of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>it</th>\n",
       "      <th>best</th>\n",
       "      <th>was</th>\n",
       "      <th>the</th>\n",
       "      <th>of</th>\n",
       "      <th>times</th>\n",
       "      <th>worst</th>\n",
       "      <th>age</th>\n",
       "      <th>wisdom</th>\n",
       "      <th>It</th>\n",
       "      <th>foolishness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   it  best  was  the  of  times  worst  age  wisdom  It  foolishness\n",
       "0   0     1    1    1   1      1      0    0       0   1            0\n",
       "1   1     0    1    1   1      1      1    0       0   0            0\n",
       "2   1     0    1    1   1      0      0    1       1   0            0\n",
       "3   1     0    1    1   1      0      0    1       0   0            1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a dataframe from the list of one hot encoded sentences\n",
    "# using the vocabulary as it's column\n",
    "\n",
    "pd.DataFrame(onehot, columns=vocabulary)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    The Similarity Matrix\n",
    "\n",
    "Calculating similarities between sentences/documents by calculating the number of common 1s at the corresponding positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# basic of the building\n",
    "\n",
    "# by iterating\n",
    "# we want to know the similarity between 2nd and 3rd sentece, thus it's set to onehot[1] and onehot[2]\n",
    "\n",
    "sim = [onehot[1][i] & onehot[2][i] for i in range(len(vocabulary))]\n",
    "display(sim)\n",
    "display(sum(sim))\n",
    "\n",
    "# or with the sumproduct approach\n",
    "# more or less like the above functions, with only one line code\n",
    "\n",
    "np.dot(onehot[1], onehot[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6, 4, 3, 3],\n",
       "       [4, 6, 4, 4],\n",
       "       [3, 4, 6, 5],\n",
       "       [3, 4, 5, 6]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate the similarity matrix using a faster approach\n",
    "# see the book page 127\n",
    "\n",
    "np.dot(onehot, np.transpose(onehot))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    One-Hot Encoding Using scikit-learn\n",
    "    \n",
    "As we already the foundation of it, there's acutally the function for vectorization from scikit-learn using **MultiLabelBinarizer**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/taufiqurrohman/opt/anaconda3/lib/python3.9/site-packages/sklearn/preprocessing/_label.py:875: UserWarning: unknown class(es) [' '] will be ignored\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1],\n",
       "       [0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1],\n",
       "       [0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1],\n",
       "       [0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "lb = MultiLabelBinarizer()  # define the function\n",
    "lb.fit(vocabulary)  # fitting to the list of dictionary we have\n",
    "lb.transform(sentences)  # transforming the list of sentences we have"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag-of-Words Models\n",
    "\n",
    "Calculating the frequency of words for each document, rather then counting whether the words from vocabularies appear or not in the document like we did before. I will use the **CountVectorizer()** from sklearn.feature_extraction.text method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_text = ['John likes to watch movies. Mary likes movies too.', \n",
    "            'Mary also likes to watch football games']\n",
    "\n",
    "sentences = sentences + new_text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Fitting the Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the way countvectorizer works\n",
    "\n",
    "# 1. learn about the vocabulary\n",
    "# for the parameter building, see the documentation on https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "\n",
    "cv.fit(sentences)\n",
    "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
    "                dtype=np.int64, encoding='utf-8', input='content',\n",
    "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
    "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
    "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
    "                tokenizer=None, vocabulary=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['age' 'also' 'best' 'foolishness' 'football' 'games' 'it' 'john' 'likes'\n",
      " 'mary' 'movies' 'of' 'the' 'times' 'to' 'too' 'was' 'watch' 'wisdom'\n",
      " 'worst']\n"
     ]
    }
   ],
   "source": [
    "# print the vocabulary\n",
    "print(cv.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Transforming the Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<6x20 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 38 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. transforming the documents to vectors\n",
    "# scikit-learn uses sparse matrix, instead of list\n",
    "\n",
    "dt = cv.transform(sentences)\n",
    "dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>also</th>\n",
       "      <th>best</th>\n",
       "      <th>foolishness</th>\n",
       "      <th>football</th>\n",
       "      <th>games</th>\n",
       "      <th>it</th>\n",
       "      <th>john</th>\n",
       "      <th>likes</th>\n",
       "      <th>mary</th>\n",
       "      <th>movies</th>\n",
       "      <th>of</th>\n",
       "      <th>the</th>\n",
       "      <th>times</th>\n",
       "      <th>to</th>\n",
       "      <th>too</th>\n",
       "      <th>was</th>\n",
       "      <th>watch</th>\n",
       "      <th>wisdom</th>\n",
       "      <th>worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  also  best  foolishness  football  games  it  john  likes  mary  \\\n",
       "0    0     0     1            0         0      0   1     0      0     0   \n",
       "1    0     0     0            0         0      0   1     0      0     0   \n",
       "2    1     0     0            0         0      0   1     0      0     0   \n",
       "3    1     0     0            1         0      0   1     0      0     0   \n",
       "4    0     0     0            0         0      0   0     1      2     1   \n",
       "5    0     1     0            0         1      1   0     0      1     1   \n",
       "\n",
       "   movies  of  the  times  to  too  was  watch  wisdom  worst  \n",
       "0       0   1    1      1   0    0    1      0       0      0  \n",
       "1       0   1    1      1   0    0    1      0       0      1  \n",
       "2       0   1    1      0   0    0    1      0       1      0  \n",
       "3       0   1    1      0   0    0    1      0       0      0  \n",
       "4       2   0    0      0   1    1    0      1       0      0  \n",
       "5       0   0    0      0   1    0    0      1       0      0  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert sparse matrix to pandas dataframe for readability\n",
    "\n",
    "pd.DataFrame(dt.toarray(), columns=cv.get_feature_names_out())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Calculating Similarities\n",
    "\n",
    "Calculating using cosine similarity, like the way we did for building a product recommendation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.83333333]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculating similarity between sentence 1 and 2\n",
    "cosine_similarity(dt[0], dt[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.833333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.524142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.524142</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5\n",
       "0  1.000000  0.833333  0.666667  0.666667  0.000000  0.000000\n",
       "1  0.833333  1.000000  0.666667  0.666667  0.000000  0.000000\n",
       "2  0.666667  0.666667  1.000000  0.833333  0.000000  0.000000\n",
       "3  0.666667  0.666667  0.833333  1.000000  0.000000  0.000000\n",
       "4  0.000000  0.000000  0.000000  0.000000  1.000000  0.524142\n",
       "5  0.000000  0.000000  0.000000  0.000000  0.524142  1.000000"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculating similarity for all sentences\n",
    "# then convert it to a dataframe\n",
    "\n",
    "pd.DataFrame(cosine_similarity(dt, dt))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    TF-IDF Models for Similarity\n",
    "\n",
    "Counting the number of total word occurrences. It will reduce weights of frequent words and at the same time increase the weights of uncommon words. We can incorporate TF-IDF to our vector using the **TfidfTransformer()** from scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>also</th>\n",
       "      <th>best</th>\n",
       "      <th>foolishness</th>\n",
       "      <th>football</th>\n",
       "      <th>games</th>\n",
       "      <th>it</th>\n",
       "      <th>john</th>\n",
       "      <th>likes</th>\n",
       "      <th>mary</th>\n",
       "      <th>movies</th>\n",
       "      <th>of</th>\n",
       "      <th>the</th>\n",
       "      <th>times</th>\n",
       "      <th>to</th>\n",
       "      <th>too</th>\n",
       "      <th>was</th>\n",
       "      <th>watch</th>\n",
       "      <th>wisdom</th>\n",
       "      <th>worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.56978</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.338027</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.338027</td>\n",
       "      <td>0.338027</td>\n",
       "      <td>0.467228</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.338027</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.338027</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.338027</td>\n",
       "      <td>0.338027</td>\n",
       "      <td>0.467228</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.338027</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.56978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.467228</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.338027</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.338027</td>\n",
       "      <td>0.338027</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.338027</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.56978</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.467228</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.56978</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.338027</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.338027</td>\n",
       "      <td>0.338027</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.338027</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.305609</td>\n",
       "      <td>0.501208</td>\n",
       "      <td>0.250604</td>\n",
       "      <td>0.611219</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250604</td>\n",
       "      <td>0.305609</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250604</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.419233</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.419233</td>\n",
       "      <td>0.419233</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.343777</td>\n",
       "      <td>0.343777</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.343777</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.343777</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        age      also     best  foolishness  football     games        it  \\\n",
       "0  0.000000  0.000000  0.56978      0.00000  0.000000  0.000000  0.338027   \n",
       "1  0.000000  0.000000  0.00000      0.00000  0.000000  0.000000  0.338027   \n",
       "2  0.467228  0.000000  0.00000      0.00000  0.000000  0.000000  0.338027   \n",
       "3  0.467228  0.000000  0.00000      0.56978  0.000000  0.000000  0.338027   \n",
       "4  0.000000  0.000000  0.00000      0.00000  0.000000  0.000000  0.000000   \n",
       "5  0.000000  0.419233  0.00000      0.00000  0.419233  0.419233  0.000000   \n",
       "\n",
       "       john     likes      mary    movies        of       the     times  \\\n",
       "0  0.000000  0.000000  0.000000  0.000000  0.338027  0.338027  0.467228   \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.338027  0.338027  0.467228   \n",
       "2  0.000000  0.000000  0.000000  0.000000  0.338027  0.338027  0.000000   \n",
       "3  0.000000  0.000000  0.000000  0.000000  0.338027  0.338027  0.000000   \n",
       "4  0.305609  0.501208  0.250604  0.611219  0.000000  0.000000  0.000000   \n",
       "5  0.000000  0.343777  0.343777  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "         to       too       was     watch   wisdom    worst  \n",
       "0  0.000000  0.000000  0.338027  0.000000  0.00000  0.00000  \n",
       "1  0.000000  0.000000  0.338027  0.000000  0.00000  0.56978  \n",
       "2  0.000000  0.000000  0.338027  0.000000  0.56978  0.00000  \n",
       "3  0.000000  0.000000  0.338027  0.000000  0.00000  0.00000  \n",
       "4  0.250604  0.305609  0.000000  0.250604  0.00000  0.00000  \n",
       "5  0.343777  0.000000  0.000000  0.343777  0.00000  0.00000  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf = TfidfTransformer()  # as always, define the model first\n",
    "tfidf_dt = tfidf.fit_transform(dt)  # fit and transform, and pass the 'dt' vector\n",
    "pd.DataFrame(tfidf_dt.toarray(), columns=cv.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.675351</td>\n",
       "      <td>0.457049</td>\n",
       "      <td>0.457049</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.675351</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.457049</td>\n",
       "      <td>0.457049</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.457049</td>\n",
       "      <td>0.457049</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.675351</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.457049</td>\n",
       "      <td>0.457049</td>\n",
       "      <td>0.675351</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.43076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.43076</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3        4        5\n",
       "0  1.000000  0.675351  0.457049  0.457049  0.00000  0.00000\n",
       "1  0.675351  1.000000  0.457049  0.457049  0.00000  0.00000\n",
       "2  0.457049  0.457049  1.000000  0.675351  0.00000  0.00000\n",
       "3  0.457049  0.457049  0.675351  1.000000  0.00000  0.00000\n",
       "4  0.000000  0.000000  0.000000  0.000000  1.00000  0.43076\n",
       "5  0.000000  0.000000  0.000000  0.000000  0.43076  1.00000"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# new cosine similarity with TF-IDF Model\n",
    "\n",
    "pd.DataFrame(cosine_similarity(tfidf_dt, tfidf_dt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real-World Application: ABC Dataset\n",
    "\n",
    "The ABC Dataset contains information on news headlines that were published over a 19-year span (until 2021). The data is taken from ABC, a respectable Australian news organization, as said (Australian Broadcasting Corporation). This notebook requires a lot (around a million records), as one of the goals of the project is to **demonstrate numerous approaches that can be applied to improve the time-efficiency of the feature engineering of the text**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publish_date</th>\n",
       "      <th>headline_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2003-02-19</td>\n",
       "      <td>aba decides against community broadcasting lic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2003-02-19</td>\n",
       "      <td>act fire witnesses must be aware of defamation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2003-02-19</td>\n",
       "      <td>a g calls for infrastructure protection summit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2003-02-19</td>\n",
       "      <td>air nz staff in aust strike for pay rise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2003-02-19</td>\n",
       "      <td>air nz strike to affect australian travellers</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  publish_date                                      headline_text\n",
       "0   2003-02-19  aba decides against community broadcasting lic...\n",
       "1   2003-02-19     act fire witnesses must be aware of defamation\n",
       "2   2003-02-19     a g calls for infrastructure protection summit\n",
       "3   2003-02-19           air nz staff in aust strike for pay rise\n",
       "4   2003-02-19      air nz strike to affect australian travellers"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of the dataframe: 1244184\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('dataset/abcnews-date-text.csv', parse_dates=['publish_date'])\n",
    "\n",
    "display(df.head(5))\n",
    "print('length of the dataframe:', len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TfidfVectorizer: library to vectorize using tf-idf the specific text, from a series\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  \n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "dt = tfidf.fit_transform(df['headline_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1244184x105966 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 8072405 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is a very large dimension of sparse matrix\n",
    "# even though the stored elemet is smaller, beware when processing this (use sampling first)\n",
    "# will take a very long time \n",
    "\n",
    "dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 38.6 s, sys: 56.1 s, total: 1min 34s\n",
      "Wall time: 2min 47s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 1.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.03076323],\n",
       "       [0.        , 0.        , 1.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , ..., 1.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 1.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.03076323, 0.        , ..., 0.        , 0.        ,\n",
       "        1.        ]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# example to show the duration it takes\n",
    "# processing the cosine similarity only for the first 100000 records\n",
    "# it already took around 2 minutes to finish\n",
    "\n",
    "cosine_similarity(dt[0:100000], dt[0:100000])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving Time-Efficiency: Reducing Feature Dimensions    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Removing stopwords    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1244184x105830 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 6730097 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "tfidf = TfidfVectorizer(stop_words=stopwords)\n",
    "dt = tfidf.fit_transform(df['headline_text'])\n",
    "dt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Minimum frequency\n",
    "\n",
    "Remove all words occuring less than twice.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1244184x64129 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 6688396 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(stop_words=stopwords, min_df=2)\n",
    "dt = tfidf.fit_transform(df['headline_text'])\n",
    "dt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Maximum frequency "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1244184x64129 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 6688396 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(stop_words=stopwords, min_df=2, max_df=0.1)\n",
    "dt = tfidf.fit_transform(df['headline_text'])\n",
    "dt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving Features: Lemmatization\n",
    "\n",
    "Instead of using the original words, we can utilize linguistic analysis, then taking the lemmatized form of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|‚ñà‚ñà‚ñé       | 72246/318573 [07:21<22:50, 179.75it/s] /Users/taufiqurrohman/opt/anaconda3/lib/python3.9/site-packages/tqdm/std.py:1195: FutureWarning: Inferring datetime64[ns] from data containing strings is deprecated and will be removed in a future version. To retain the old behavior explicitly pass Series(data, dtype=datetime64[ns])\n",
      "  for obj in iterable:\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 318573/318573 [33:46<00:00, 157.22it/s] \n"
     ]
    }
   ],
   "source": [
    "# beware of running this code\n",
    "# will take some time\n",
    "# to track the progress, wrap the for-loops using tqdm\n",
    "\n",
    "# for this project, we will restrict the data from the news of 2015 and later\n",
    "\n",
    "df_up_2015 = df[df['publish_date'] > '2015-01-01']\n",
    "df_up_2015.reset_index(inplace=True)\n",
    "\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "pos_to_take = ['NOUN', 'PROPN', 'ADJ', 'ADV', 'VERB']\n",
    "\n",
    "for i, row in tqdm(df_up_2015.iterrows(), total=df_up_2015.shape[0]):  # iterating dataframe row with its index\n",
    "    doc = nlp(str(row['headline_text']))\n",
    "    df_up_2015.at[i, 'lemmas'] = ' '.join([token.lemma_ for token in doc])  # instead of 'df.loc', we can use df.at to slice only one row\n",
    "    df_up_2015.at[i, 'nav'] = ' '.join([token.lemma_ for token in doc if token.pos_ in pos_to_take])  # this one as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publish_date</th>\n",
       "      <th>headline_text</th>\n",
       "      <th>lemmas</th>\n",
       "      <th>nav</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-01-02</td>\n",
       "      <td>abalone salmon fish farming environment</td>\n",
       "      <td>abalone salmon fish farming environment</td>\n",
       "      <td>abalone salmon fish farming environment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-01-02</td>\n",
       "      <td>abalone salmon tassal huon aquaculture aquacul...</td>\n",
       "      <td>abalone salmon tassal huon aquaculture aquacul...</td>\n",
       "      <td>abalone salmon tassal huon aquaculture aquacul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-01-02</td>\n",
       "      <td>act government approves changes to asbestos ma...</td>\n",
       "      <td>act government approve change to asbestos mana...</td>\n",
       "      <td>act government approve change asbestos managem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-01-02</td>\n",
       "      <td>activist to send copies of the interview to no...</td>\n",
       "      <td>activist to send copy of the interview to nort...</td>\n",
       "      <td>activist send copy interview north korea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-01-02</td>\n",
       "      <td>agricultural graduate job access limited by la...</td>\n",
       "      <td>agricultural graduate job access limit by lack...</td>\n",
       "      <td>agricultural graduate job access limit lack ex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318568</th>\n",
       "      <td>2021-12-31</td>\n",
       "      <td>two aged care residents die as state records 2...</td>\n",
       "      <td>two aged care resident die as state record 2;093</td>\n",
       "      <td>aged care resident die state record</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318569</th>\n",
       "      <td>2021-12-31</td>\n",
       "      <td>victoria records 5;919 new cases and seven deaths</td>\n",
       "      <td>victoria record 5;919 new case and seven death</td>\n",
       "      <td>victoria record new case death</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318570</th>\n",
       "      <td>2021-12-31</td>\n",
       "      <td>wa delays adopting new close contact definition</td>\n",
       "      <td>wa delay adopt new close contact definition</td>\n",
       "      <td>wa delay adopt new close contact definition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318571</th>\n",
       "      <td>2021-12-31</td>\n",
       "      <td>western ringtail possums found badly dehydrate...</td>\n",
       "      <td>western ringtail possum find badly dehydrate i...</td>\n",
       "      <td>western ringtail possum find badly dehydrate h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318572</th>\n",
       "      <td>2021-12-31</td>\n",
       "      <td>what makes you a close covid contact here are ...</td>\n",
       "      <td>what make you a close covid contact here be th...</td>\n",
       "      <td>make close covid contact here new rule</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>318573 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       publish_date                                      headline_text  \\\n",
       "0        2015-01-02            abalone salmon fish farming environment   \n",
       "1        2015-01-02  abalone salmon tassal huon aquaculture aquacul...   \n",
       "2        2015-01-02  act government approves changes to asbestos ma...   \n",
       "3        2015-01-02  activist to send copies of the interview to no...   \n",
       "4        2015-01-02  agricultural graduate job access limited by la...   \n",
       "...             ...                                                ...   \n",
       "318568   2021-12-31  two aged care residents die as state records 2...   \n",
       "318569   2021-12-31  victoria records 5;919 new cases and seven deaths   \n",
       "318570   2021-12-31    wa delays adopting new close contact definition   \n",
       "318571   2021-12-31  western ringtail possums found badly dehydrate...   \n",
       "318572   2021-12-31  what makes you a close covid contact here are ...   \n",
       "\n",
       "                                                   lemmas  \\\n",
       "0                 abalone salmon fish farming environment   \n",
       "1       abalone salmon tassal huon aquaculture aquacul...   \n",
       "2       act government approve change to asbestos mana...   \n",
       "3       activist to send copy of the interview to nort...   \n",
       "4       agricultural graduate job access limit by lack...   \n",
       "...                                                   ...   \n",
       "318568   two aged care resident die as state record 2;093   \n",
       "318569     victoria record 5;919 new case and seven death   \n",
       "318570        wa delay adopt new close contact definition   \n",
       "318571  western ringtail possum find badly dehydrate i...   \n",
       "318572  what make you a close covid contact here be th...   \n",
       "\n",
       "                                                      nav  \n",
       "0                 abalone salmon fish farming environment  \n",
       "1       abalone salmon tassal huon aquaculture aquacul...  \n",
       "2       act government approve change asbestos managem...  \n",
       "3                activist send copy interview north korea  \n",
       "4       agricultural graduate job access limit lack ex...  \n",
       "...                                                   ...  \n",
       "318568                aged care resident die state record  \n",
       "318569                     victoria record new case death  \n",
       "318570        wa delay adopt new close contact definition  \n",
       "318571  western ringtail possum find badly dehydrate h...  \n",
       "318572             make close covid contact here new rule  \n",
       "\n",
       "[318573 rows x 4 columns]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_up_2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(318573, 69905)\n",
      "19475544\n",
      "(318573, 58427)\n",
      "15420656\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "dt = tfidf.fit_transform(df_up_2015['headline_text'].map(str))  # without lemmatized form\n",
    "print(dt.shape)\n",
    "print(dt.data.nbytes)\n",
    "\n",
    "tfidf = TfidfVectorizer(stop_words=stopwords)\n",
    "dt = tfidf.fit_transform(df_up_2015['nav'].map(str))  # with lemmatized form\n",
    "print(dt.shape)\n",
    "print(dt.data.nbytes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Removing Most Common Words\n",
    "\n",
    "Using <a href='https://github.com/first20hours/google-10000-english/blob/master/google-10000-english.txt'>Google dataset</a> of 10,000 most common words in English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<318573x51206 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 479932 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we save elements stored a lot by this process\n",
    "\n",
    "common_words = pd.read_csv('dataset/google-10000-english.txt', header=None)\n",
    "common_words = set(common_words.iloc[:,0].values)\n",
    "\n",
    "tfidf = TfidfVectorizer(stop_words=common_words)\n",
    "dt = tfidf.fit_transform(df_up_2015['nav'].map(str))\n",
    "dt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Adding Context via N-Grams    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(318573, 221206)\n",
      "22983976\n",
      "(318573, 286281)\n",
      "24720640\n"
     ]
    }
   ],
   "source": [
    "# 2 n-grams\n",
    "tfidf = TfidfVectorizer(stop_words=stopwords, ngram_range=(1,2), min_df=2)\n",
    "dt = tfidf.fit_transform(df_up_2015['headline_text'])\n",
    "print(dt.shape)\n",
    "print(dt.data.nbytes)\n",
    "\n",
    "# 3 n-grams\n",
    "tfidf = TfidfVectorizer(stop_words=stopwords, ngram_range=(1,3), min_df=2)\n",
    "dt = tfidf.fit_transform(df_up_2015['headline_text'])\n",
    "print(dt.shape)\n",
    "print(dt.data.nbytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(318573, 46834)\n",
      "4261056\n"
     ]
    }
   ],
   "source": [
    "# combining ngrams with linguistic features + most common words\n",
    "# reduce the len of dt by factor 6\n",
    "\n",
    "tfidf = TfidfVectorizer(stop_words=common_words, ngram_range=(1,2), min_df=2)\n",
    "dt = tfidf.fit_transform(df_up_2015['nav'])\n",
    "print(dt.shape)\n",
    "print(dt.data.nbytes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Business Application: After Vectorizing\n",
    "\n",
    "For this, we will only use the vector of the text with below specifications:\n",
    "- Original dataframe, not preprocessed using linguistic process (lemmatized and pos tags)\n",
    "- Removed the stopwords\n",
    "- Minimum of the word appearing in the data: 2 times\n",
    "- Normalization using L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(stop_words=stopwords, \n",
    "                        ngram_range=(1,2), \n",
    "                        min_df=2, \n",
    "                        norm='l2')\n",
    "\n",
    "dt = tfidf.fit_transform(df['headline_text'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Finding Similar Headlines\n",
    "\n",
    "Looking for a made-up headline that is most similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, we transform the headline that we're going to check\n",
    "text_to_look_for = 'Lionel Messi and the world cup'\n",
    "look_for = tfidf.transform([text_to_look_for])\n",
    "\n",
    "# second, calculate the cosine similarity between look_for and dt\n",
    "sim = cosine_similarity(look_for, dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publish_date</th>\n",
       "      <th>headline_text</th>\n",
       "      <th>cosine_sim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1004486</th>\n",
       "      <td>2016-01-12</td>\n",
       "      <td>lionel messi in quotes</td>\n",
       "      <td>0.753968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1129253</th>\n",
       "      <td>2018-06-01</td>\n",
       "      <td>lionel messi montage</td>\n",
       "      <td>0.721831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1211510</th>\n",
       "      <td>2020-08-26</td>\n",
       "      <td>could lionel messi be coming to your club</td>\n",
       "      <td>0.715942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1150146</th>\n",
       "      <td>2018-12-09</td>\n",
       "      <td>young afghan lionel messi fan threatened by ta...</td>\n",
       "      <td>0.624409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1129594</th>\n",
       "      <td>2018-06-04</td>\n",
       "      <td>world cup journeys lionel messi chance to matc...</td>\n",
       "      <td>0.608173</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        publish_date                                      headline_text  \\\n",
       "1004486   2016-01-12                             lionel messi in quotes   \n",
       "1129253   2018-06-01                               lionel messi montage   \n",
       "1211510   2020-08-26          could lionel messi be coming to your club   \n",
       "1150146   2018-12-09  young afghan lionel messi fan threatened by ta...   \n",
       "1129594   2018-06-04  world cup journeys lionel messi chance to matc...   \n",
       "\n",
       "         cosine_sim  \n",
       "1004486    0.753968  \n",
       "1129253    0.721831  \n",
       "1211510    0.715942  \n",
       "1150146    0.624409  \n",
       "1129594    0.608173  "
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# last one, show the most similar headlines from dt\n",
    "\n",
    "top_n = 5  # change top_n here by the number of top similar news you want to see\n",
    "x = np.argsort(sim)[0][::-1][:top_n]\n",
    "top_df = df[['publish_date', 'headline_text']].iloc[x]\n",
    "for i in x:\n",
    "    top_df.loc[i, 'cosine_sim'] = sim[0][i]\n",
    "\n",
    "top_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then, we can wrap it to a function\n",
    "\n",
    "def look_for_text(doc, text_to_look_for, top_n, **kwargs):\n",
    "    '''\n",
    "    Function to look for the most similar text from a vectorized data text\n",
    "    \n",
    "    Parameters:\n",
    "    doc                 : collection of docs we want to analyze (from a corpus)\n",
    "    text_to_look_for    : text that we're looking for\n",
    "    dt                  : data frame that has been vectorized\n",
    "    top_n               : n top similar text we're looking for\n",
    "    **kwargs            : pass the parameters from TfidfVectorizer\n",
    "    \n",
    "    Returns:\n",
    "    Dataframe of top similar text with cosine similarity method\n",
    "    '''\n",
    "    \n",
    "    tfidf = TfidfVectorizer(**kwargs)\n",
    "    dt = tfidf.fit_transform(doc)\n",
    "    \n",
    "    look_for = tfidf.transform([text_to_look_for])\n",
    "    sim = cosine_similarity(look_for, dt)\n",
    "    \n",
    "    x = np.argsort(sim)[0][::-1][:top_n]\n",
    "    top_df = df[['publish_date', 'headline_text']].iloc[x]\n",
    "    for i in x:\n",
    "        top_df.loc[i, 'cosine_sim'] = sim[0][i]\n",
    "    \n",
    "    return top_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publish_date</th>\n",
       "      <th>headline_text</th>\n",
       "      <th>cosine_sim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>722039</th>\n",
       "      <td>2012-09-05</td>\n",
       "      <td>australia and indonesia at odds over rescued</td>\n",
       "      <td>0.671871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>826204</th>\n",
       "      <td>2013-10-15</td>\n",
       "      <td>australia and indonesia split with activists over</td>\n",
       "      <td>0.666306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245606</th>\n",
       "      <td>2006-06-27</td>\n",
       "      <td>australia and indonesia back on track</td>\n",
       "      <td>0.637413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>834743</th>\n",
       "      <td>2013-11-18</td>\n",
       "      <td>australia indonesia relationship</td>\n",
       "      <td>0.589054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>896768</th>\n",
       "      <td>2014-08-19</td>\n",
       "      <td>australia and indonesia sort out strained</td>\n",
       "      <td>0.575680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1093584</th>\n",
       "      <td>2017-08-10</td>\n",
       "      <td>australia and indonesia at the asean summit</td>\n",
       "      <td>0.555942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1157287</th>\n",
       "      <td>2019-02-28</td>\n",
       "      <td>australia indonesia to sign free trade agreement</td>\n",
       "      <td>0.539515</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        publish_date                                      headline_text  \\\n",
       "722039    2012-09-05       australia and indonesia at odds over rescued   \n",
       "826204    2013-10-15  australia and indonesia split with activists over   \n",
       "245606    2006-06-27              australia and indonesia back on track   \n",
       "834743    2013-11-18                   australia indonesia relationship   \n",
       "896768    2014-08-19          australia and indonesia sort out strained   \n",
       "1093584   2017-08-10        australia and indonesia at the asean summit   \n",
       "1157287   2019-02-28   australia indonesia to sign free trade agreement   \n",
       "\n",
       "         cosine_sim  \n",
       "722039     0.671871  \n",
       "826204     0.666306  \n",
       "245606     0.637413  \n",
       "834743     0.589054  \n",
       "896768     0.575680  \n",
       "1093584    0.555942  \n",
       "1157287    0.539515  "
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's try the function\n",
    "\n",
    "text_to_look_for = 'Australia and Indonesia agreement'\n",
    "look_for_text(df['headline_text'], \n",
    "              text_to_look_for, \n",
    "              top_n=7,\n",
    "              stop_words=stopwords, \n",
    "              ngram_range=(1,2), \n",
    "              min_df=2, \n",
    "              norm='l2')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Finding the Two Most Similar Documents in a Corpus\n",
    "\n",
    "This function will be taken directly from the book as it **has something to do with time-efficiency**. See more on **PAGE 146** for more detail information on how to do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 125/125 [10:34<00:00,  5.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9min 31s, sys: 17.3 s, total: 9min 48s\n",
      "Wall time: 10min 34s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "batch = 10000\n",
    "max_sim = 0.0\n",
    "\n",
    "max_a = None\n",
    "max_b = None\n",
    "\n",
    "for a in tqdm(range(0, dt.shape[0], batch)):\n",
    "    \n",
    "    for b in range(0, a+batch, batch): \n",
    "        # print(a, b) -> should be eliminated, the book says to print(a,b)\n",
    "        r = np.dot(dt[a:a+batch], np.transpose(dt[b:b+batch]))\n",
    "        \n",
    "        # eliminate identical vectors\n",
    "        # by setting their similarity to np.nan which gets sorted out r[r > 0.9999] = np.nan\n",
    "        r[r > 0.9999] = np.nan\n",
    "        sim = r.max()\n",
    "        \n",
    "        if sim > max_sim:\n",
    "            # argmax returns a single value which we have to\n",
    "            # map to the two dimensions\n",
    "            (max_a, max_b) = np.unravel_index(np.argmax(r), r.shape)  \n",
    "            \n",
    "            # adjust offsets in corpus (this is a submatrix)\n",
    "            max_a += a\n",
    "            max_b += b\n",
    "            max_sim = sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrap into a function\n",
    "# beware of the how long it takes to process\n",
    "\n",
    "def similar_doc_in_corpus(doc, **kwargs):\n",
    "    batch = 10000\n",
    "    max_sim = 0.0\n",
    "\n",
    "    max_a = None\n",
    "    max_b = None\n",
    "    \n",
    "    tfidf = TfidfVectorizer(**kwargs)\n",
    "    dt = tfidf.fit_transform(doc)\n",
    "\n",
    "    for a in tqdm(range(0, dt.shape[0], batch)):\n",
    "    \n",
    "        for b in range(0, a+batch, batch): \n",
    "            # print(a, b) -> should be eliminated, the book says to print(a,b)\n",
    "            r = np.dot(dt[a:a+batch], np.transpose(dt[b:b+batch]))\n",
    "        \n",
    "            # eliminate identical vectors\n",
    "            # by setting their similarity to np.nan which gets sorted out r[r > 0.9999] = np.nan\n",
    "            r[r > 0.9999] = np.nan\n",
    "            sim = r.max()\n",
    "        \n",
    "            if sim > max_sim:\n",
    "                # argmax returns a single value which we have to\n",
    "                # map to the two dimensions\n",
    "                (max_a, max_b) = np.unravel_index(np.argmax(r), r.shape)  \n",
    "            \n",
    "                # adjust offsets in corpus (this is a submatrix)\n",
    "                max_a += a\n",
    "                max_b += b\n",
    "                max_sim = sim\n",
    "        \n",
    "    list_sim = [df.iloc[max_a]['headline_text'], df.iloc[max_b]['headline_text']]\n",
    "        \n",
    "    return print('\\n'.join((list_sim)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 125/125 [13:14<00:00,  6.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vline fails to meet punctuality targets report\n",
      "vline fails to meet punctuality targets\n"
     ]
    }
   ],
   "source": [
    "# let's check the most similar sentences here\n",
    "similar_doc_in_corpus(df['headline_text'], \n",
    "                      stop_words=stopwords, \n",
    "                      ngram_range=(1,2), \n",
    "                      min_df=2, \n",
    "                      norm='l2')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Finding Related Words in the Corpus\n",
    "\n",
    "Based by two rules:\n",
    "\n",
    "- Words are related if they're used in the same documents\n",
    "- It becomes more related if they frequently appear together in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the term-document matrix\n",
    "# it's the transposed form of the document-term matrix\n",
    "\n",
    "tfidf_word = TfidfVectorizer(stop_words=stopwords, min_df=1000)\n",
    "dt_word = tfidf_word.fit_transform(df['headline_text'])\n",
    "\n",
    "r = cosine_similarity(dt_word.T, dt_word.T)  # this is the part where we transpose the data\n",
    "np.fill_diagonal(r, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kong related to hong\n",
      "sri related to lanka\n",
      "covid related to 19\n",
      "seekers related to asylum\n",
      "springs related to alice\n",
      "trump related to donald\n",
      "hour related to country\n",
      "pleads related to guilty\n",
      "hill related to broken\n",
      "vs related to summary\n",
      "violence related to domestic\n",
      "climate related to change\n",
      "royal related to commission\n",
      "care related to aged\n",
      "scott related to morrison\n",
      "gold related to coast\n",
      "driving related to drink\n",
      "wall related to street\n",
      "mental related to health\n",
      "world related to cup\n"
     ]
    }
   ],
   "source": [
    "voc = tfidf_word.get_feature_names_out()  # create the vocabulary\n",
    "size = r.shape[0]\n",
    "\n",
    "for i in np.argsort(r.flatten())[::-1][0:40]:\n",
    "    # finding the pair\n",
    "    a = int(i/size)\n",
    "    b = i%size\n",
    "    if a > b:  # to avoid repetitions (only show the pair once)\n",
    "        print(f'{voc[a]} related to {voc[b]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can make it into a dataframe and show it's cosine similarity score\n",
    "\n",
    "def similar_word(doc, top_n, min_appear, **kwargs):\n",
    "    '''\n",
    "    Function to look for the most similar word from a corpus\n",
    "    \n",
    "    Parameters:\n",
    "    doc         : collection of docs we want to analyze (from a corpus)\n",
    "    top_n       : n top similar text we're looking for\n",
    "    min_n       : minimal appearence of the pair\n",
    "    **kwargs    : pass the parameters from TfidfVectorizer\n",
    "    \n",
    "    Returns:\n",
    "    Dataframe of top similar word from a corpus with it's cosine similarity\n",
    "    '''\n",
    "    \n",
    "    tfidf_word = TfidfVectorizer(min_df=min_appear, **kwargs)  \n",
    "    dt_word = tfidf_word.fit_transform(doc)\n",
    "\n",
    "    r = cosine_similarity(dt_word.T, dt_word.T)  # this is the part where we transpose the data\n",
    "    np.fill_diagonal(r, 0)\n",
    "    \n",
    "    voc = tfidf_word.get_feature_names_out()  # create the vocabulary\n",
    "    size = r.shape[0]\n",
    "    \n",
    "    # create the data frame and its row iteration\n",
    "    df_sim = pd.DataFrame()\n",
    "    row = 0\n",
    "    \n",
    "    for i in np.argsort(r.flatten())[::-1][0:100]:\n",
    "        # finding the pair\n",
    "        a = int(i/size)\n",
    "        b = i%size\n",
    "        if a > b:  # to avoid repetitions (only show the pair once)\n",
    "            df_sim.loc[row, 'word_1'] = voc[a]\n",
    "            df_sim.loc[row, 'word_2'] = voc[b]\n",
    "            df_sim.loc[row, 'sim'] = r[a][b]\n",
    "            row += 1\n",
    "    \n",
    "    return df_sim.head(top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_1</th>\n",
       "      <th>word_2</th>\n",
       "      <th>sim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kong</td>\n",
       "      <td>hong</td>\n",
       "      <td>0.953522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sri</td>\n",
       "      <td>lanka</td>\n",
       "      <td>0.817174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>covid</td>\n",
       "      <td>19</td>\n",
       "      <td>0.630267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>seekers</td>\n",
       "      <td>asylum</td>\n",
       "      <td>0.579449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>springs</td>\n",
       "      <td>alice</td>\n",
       "      <td>0.575388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>trump</td>\n",
       "      <td>donald</td>\n",
       "      <td>0.571107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>hour</td>\n",
       "      <td>country</td>\n",
       "      <td>0.536050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>pleads</td>\n",
       "      <td>guilty</td>\n",
       "      <td>0.516256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>hill</td>\n",
       "      <td>broken</td>\n",
       "      <td>0.479397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>vs</td>\n",
       "      <td>summary</td>\n",
       "      <td>0.448103</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    word_1   word_2       sim\n",
       "0     kong     hong  0.953522\n",
       "1      sri    lanka  0.817174\n",
       "2    covid       19  0.630267\n",
       "3  seekers   asylum  0.579449\n",
       "4  springs    alice  0.575388\n",
       "5    trump   donald  0.571107\n",
       "6     hour  country  0.536050\n",
       "7   pleads   guilty  0.516256\n",
       "8     hill   broken  0.479397\n",
       "9       vs  summary  0.448103"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_word(df['headline_text'], \n",
    "             top_n=10, \n",
    "             min_appear=1000, \n",
    "             stop_words=stopwords)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0fd9779e8d8f386467a1f1d1102cd9c30d79c45b15374f3d42e7180eb33a0483"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
