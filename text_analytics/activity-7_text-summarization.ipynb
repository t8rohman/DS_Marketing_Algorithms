{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Summarization: Sport News and Travel Forum Discussion Data\n",
    "\n",
    "Project to summarize the text. Instead of abstractive methods, this project will use **Extractive Summarization**, by giving important phrases and setences weight to form a summary of an entire text. Functions referred to Blueprints for Text Analytics by Albrecht et al. (2021) with several adjustments to make it more clear. For the ready-to-use functions, please refer to file **fun_nlp_spacy_text.py.** T\n",
    "\n",
    "There are several steps at least until we get our summarized text:\n",
    "\n",
    "1. Create an intermediate representation of the text\n",
    "2. Score the sentences/phrases based on the chosen representation\n",
    "3. Rank and choose sentences to create a summary of the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import regex as re\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk import tokenize\n",
    "import spacy\n",
    "\n",
    "from newspaper import Article\n",
    "import reprlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Reading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "from newspaper import Article\n",
    " \n",
    "url = \"https://www.mirror.co.uk/sport/football/transfer-news/lionel-messi-sergio-ramos-psg-29440311\"\n",
    " \n",
    "# download and parse article\n",
    "article = Article(url)\n",
    "article.download()\n",
    "article.parse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Paris Saint-Germain have put contract talks with Lionel Messi and Sergio Ramos on hold amid threats of UEFA sanctions which could hurt their Champions League status\\n\\nParis Saint-Germain have put contract talks with Lionel Messi and Sergio Ramos on hold amid concerns they will face fresh Champions League sanctions for further Financial Fair Play (FFP) breaches.\\n\\nPSG were one of 10 clubs fine...til definitive decisions have been reached.\\n\\nAlongside Mbappe’s new deal last year, PSG also signed Portugal international midfield duo Renato Sanches and Vitinha from Lille and Porto respectively, alongside midfielder Carlos Soler from Valencia. They also added striker Hugo Ekitike (Reims) and defender Nordi Mukiele (RB Leipzig) while making Nuno Mendes’s loan move from Sporting CP permanent.'\n"
     ]
    }
   ],
   "source": [
    "import reprlib  # to limit printing the text up to a certain number of string\n",
    "\n",
    "r = reprlib.Repr()\n",
    "r.maxstring = 800  # set the number of string here\n",
    "\n",
    "print(r.repr(article.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete \\n\\n from the text\n",
    "\n",
    "sentence = str(article.text)\n",
    "clean_text = ''\n",
    "tit = 0\n",
    "\n",
    "for i, char in enumerate(article.text):\n",
    "    if char == \"\\n\" and sentence[i+1] == \"\\n\" and sentence[i-1] != \".\" :\n",
    "        clean_text += \". \"\n",
    "    elif char == \"\\n\" and sentence[i+1] != \"\\n\":\n",
    "        clean_text += \" \"\n",
    "    elif char != \"\\n\":\n",
    "        clean_text += char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Paris Saint-Germain have put contract talks with Lionel Messi and Sergio Ramos on hold amid threats of UEFA sanctions which could hurt their Champions League status.  Paris Saint-Germain have put contract talks with Lionel Messi and Sergio Ramos on hold amid concerns they will face fresh Champions League sanctions for further Financial Fair Play (FFP) breaches. PSG were one of 10 clubs fined for breaching UEFA’s Financial Fair Play (FFP) rules for the 2020-21 season. They paid out a €10million fine while having a further €45m suspended pending future accounts, with further punishments possible. PSG have been posting record losses in recent years with their most recent set of accounts showing €370m in losses. Messi and Ramos are among their highest earning stars and with both out of contract this summer, the club must make tough financial decisions. Possible sanctions could include the French club being unable to register any new players to their European squad for next season and a possible reduction in the size of their registered squad for European competition dropping from 25 to 23. There is also a distant threat that the club could be expelled from European competition entirely, as reported by Diario AS. The soonest that could happen would be by the 2024/25 campaign and would be as the result of sustained losses in that timeframe – meaning that their wage bill needs to be cut drastically in the meantime. Prior to the club tying down Kylian Mbappe to a new three-year contract extension last summer, Messi was the best-paid player in the world. The Argentine’s first year in Paris saw him take home an annual salary of £25.5million (€30m), although this increased to £34million (€40m) for the current and final year of his two-year deal in Paris. It is claimed that Messi has received more than €1,000m throughout his career – more than any other player. Image: Tim Clayton/Corbis via Getty Images) Tim Clayton/Corbis via Getty Images).  Have Your Say! What should PSG do over the contract situations for Messi and Ramos? Tell us what you think here. Like Messi, Ramos is out of contract this summer. Now aged 36, Ramos impressed in PSG’s Champions League elimination this week but the club have a deal lined up to sign Milan Skriniar from Inter while they want to sign Pau Torres from Villarreal. That may mean Ramos, barring a drastic wage reduction, is moved on. A report from L’Équipe has outlined how PSG want to keep Messi and Ramos at the club with informal discussions opened but the FFP pressures and threat of sanctions may mean new deals are not feasible. The club are aware of an urgent need to restructure their squad and finances, so new contracts have been put on hold until definitive decisions have been reached. Alongside Mbappe’s new deal last year, PSG also signed Portugal international midfield duo Renato Sanches and Vitinha from Lille and Porto respectively, alongside midfielder Carlos Soler from Valencia. They also added striker Hugo Ekitike (Reims) and defender Nordi Mukiele (RB Leipzig) while making Nuno Mendes’s loan move from Sporting CP permanent.'"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if we still have \\n\n",
    "\n",
    "re.findall('\\n', clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1 - Identifying Important Words with TF-IDF Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk import tokenize\n",
    "\n",
    "# tokenize by sentence\n",
    "# and transform it using TfidfTransformizer\n",
    "\n",
    "sentences = tokenize.sent_tokenize(clean_text)\n",
    "tfidfVectorizer = TfidfVectorizer()\n",
    "words_tfidf = tfidfVectorizer.fit_transform(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the sentence in descending order by tfidf sum value\n",
    "\n",
    "sent_sum = words_tfidf.sum(axis=1)\n",
    "important_sent = np.argsort(sent_sum, axis=0)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paris Saint-Germain have put contract talks with Lionel Messi and Sergio Ramos on hold amid concerns they will face fresh Champions League sanctions for further Financial Fair Play (FFP) breaches.\n",
      "Now aged 36, Ramos impressed in PSG’s Champions League elimination this week but the club have a deal lined up to sign Milan Skriniar from Inter while they want to sign Pau Torres from Villarreal.\n",
      "A report from L’Équipe has outlined how PSG want to keep Messi and Ramos at the club with informal discussions opened but the FFP pressures and threat of sanctions may mean new deals are not feasible.\n"
     ]
    }
   ],
   "source": [
    "# print the most important sentences in order they appear\n",
    "# set how many sentences from num_sum_sent parameter\n",
    "\n",
    "num_sum_sent = 3\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    if i in important_sent[:num_sum_sent]:\n",
    "        print(sentences[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make it into a reproducible function\n",
    "\n",
    "def tfidf_summary(text, num_sum_sent):\n",
    "    sum_list = []\n",
    "    \n",
    "    sentences = tokenize.sent_tokenize(text)\n",
    "    tfidfVectorizer = TfidfVectorizer()\n",
    "    words_tfidf = tfidfVectorizer.fit_transform(sentences)\n",
    "    \n",
    "    sent_sum = words_tfidf.sum(axis=1)\n",
    "    important_sent = np.argsort(sent_sum, axis=0)[::-1]\n",
    "    \n",
    "    for i in range(len(sentences)):\n",
    "        if i in important_sent[:num_sum_sent]:\n",
    "            sum_list.append(sentences[i])\n",
    "            \n",
    "    return sum_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Paris Saint-Germain have put contract talks with Lionel Messi and Sergio Ramos on hold amid concerns they will face fresh Champions League sanctions for further Financial Fair Play (FFP) breaches.',\n",
       " 'Now aged 36, Ramos impressed in PSG’s Champions League elimination this week but the club have a deal lined up to sign Milan Skriniar from Inter while they want to sign Pau Torres from Villarreal.',\n",
       " 'A report from L’Équipe has outlined how PSG want to keep Messi and Ramos at the club with informal discussions opened but the FFP pressures and threat of sanctions may mean new deals are not feasible.']"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_sum = tfidf_summary(clean_text, 3)\n",
    "tfidf_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2 - LSA Algorithm\n",
    "\n",
    "Instead of building from the scratch, this one will use LSA library, <a href='https://miso-belica.github.io/sumy/'>Sumy, made Michal Belica.</a> It makes the summarization using LSA method easier by only following 4 steps:\n",
    "\n",
    "1. Tokenize the string using Tokenizer()\n",
    "2. Create the document using PlaintextParser()\n",
    "3. Stemmer(), to normalize the words into the single one\n",
    "4. Summarizer(), to summarize the document (with combination of taking out stop words for better result), we can choose the method as well here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.nlp.stemmers import Stemmer\n",
    "from sumy.utils import get_stop_words\n",
    "\n",
    "from sumy.summarizers.lsa import LsaSummarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = 'english'\n",
    "stemmer = Stemmer(lang)\n",
    "\n",
    "parser = PlaintextParser.from_string(clean_text, Tokenizer(lang))\n",
    "summarizer = LsaSummarizer(stemmer)  # we can actually choose the summarizer method here, by changing the method\n",
    "summarizer.stop_words = get_stop_words(lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paris Saint-Germain have put contract talks with Lionel Messi and Sergio Ramos on hold amid threats of UEFA sanctions which could hurt their Champions League status.\n",
      "Paris Saint-Germain have put contract talks with Lionel Messi and Sergio Ramos on hold amid concerns they will face fresh Champions League sanctions for further Financial Fair Play (FFP) breaches.\n",
      "They also added striker Hugo Ekitike (Reims) and defender Nordi Mukiele (RB Leipzig) while making Nuno Mendes’s loan move from Sporting CP permanent.\n"
     ]
    }
   ],
   "source": [
    "num_sum_sent = 3\n",
    "\n",
    "for sentence in summarizer(parser.document, num_sum_sent):\n",
    "    print(str(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make it into a reproducible function\n",
    "\n",
    "def lsa_summary(text, num_sum_sent, lang):\n",
    "    sum_list = []\n",
    "    lang = lang\n",
    "    stemmer = Stemmer(lang)\n",
    "\n",
    "    parser = PlaintextParser.from_string(text, Tokenizer(lang))\n",
    "    summarizer = LsaSummarizer(stemmer)  # we can actually choose the summarizer method here, by changing the method\n",
    "    summarizer.stop_words = get_stop_words(lang)\n",
    "    \n",
    "    for sentence in summarizer(parser.document, num_sum_sent):\n",
    "        sum_list.append(str(sentence))\n",
    "    \n",
    "    return sum_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Paris Saint-Germain have put contract talks with Lionel Messi and Sergio Ramos on hold amid threats of UEFA sanctions which could hurt their Champions League status.',\n",
       " 'Paris Saint-Germain have put contract talks with Lionel Messi and Sergio Ramos on hold amid concerns they will face fresh Champions League sanctions for further Financial Fair Play (FFP) breaches.',\n",
       " 'They also added striker Hugo Ekitike (Reims) and defender Nordi Mukiele (RB Leipzig) while making Nuno Mendes’s loan move from Sporting CP permanent.']"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsa_sum = lsa_summary(clean_text, 3, 'english')\n",
    "lsa_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 3 - Indicator Representation (Page Rank)\n",
    "\n",
    "Page Rank method derived from Google method to give rank to their web pages. For example, if a webpage 'X' links to webpage 'W', 'X' contributes to the importance of 'W,' according to PageRank, which assumes that the rank of a webpage W depends on the value of a webpage provided by other web pages in terms of connections to the page. We can assume webpage as sentence. More, check this article written by Mehul Gupta: <a href='https://medium.com/data-science-in-your-pocket/text-summarization-using-textrank-in-nlp-4bce52c5b390'>Text summarization using TextRank in NLP</a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can use sumy as well to use the page rank method\n",
    "\n",
    "from sumy.summarizers.text_rank import TextRankSummarizer\n",
    "\n",
    "parser = PlaintextParser.from_string(clean_text, Tokenizer(lang))\n",
    "summarizer = TextRankSummarizer(stemmer)  # this is the example we can change summarizer method, from LsaSummarizer to TextRank\n",
    "summarizer.stop_words = get_stop_words(lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paris Saint-Germain have put contract talks with Lionel Messi and Sergio Ramos on hold amid threats of UEFA sanctions which could hurt their Champions League status.\n",
      "Paris Saint-Germain have put contract talks with Lionel Messi and Sergio Ramos on hold amid concerns they will face fresh Champions League sanctions for further Financial Fair Play (FFP) breaches.\n",
      "A report from L’Équipe has outlined how PSG want to keep Messi and Ramos at the club with informal discussions opened but the FFP pressures and threat of sanctions may mean new deals are not feasible.\n"
     ]
    }
   ],
   "source": [
    "num_sum_sent = 3\n",
    "\n",
    "for sentence in summarizer(parser.document, num_sum_sent):\n",
    "    print(str(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make it into a reproducible function\n",
    "\n",
    "def pagerank_summary(text, num_sum_sent, lang):\n",
    "    sum_list = []\n",
    "    lang = lang\n",
    "    \n",
    "    parser = PlaintextParser.from_string(clean_text, Tokenizer(lang))\n",
    "    summarizer = TextRankSummarizer(stemmer)  # this is the example we can change summarizer method, from LsaSummarizer to TextRank\n",
    "    summarizer.stop_words = get_stop_words(lang)\n",
    "    \n",
    "    for sentence in summarizer(parser.document, num_sum_sent):\n",
    "        sum_list.append(str(sentence))\n",
    "    \n",
    "    return sum_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Paris Saint-Germain have put contract talks with Lionel Messi and Sergio Ramos on hold amid threats of UEFA sanctions which could hurt their Champions League status.',\n",
       " 'Paris Saint-Germain have put contract talks with Lionel Messi and Sergio Ramos on hold amid concerns they will face fresh Champions League sanctions for further Financial Fair Play (FFP) breaches.',\n",
       " 'A report from L’Équipe has outlined how PSG want to keep Messi and Ramos at the club with informal discussions opened but the FFP pressures and threat of sanctions may mean new deals are not feasible.']"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pagerank_sum = pagerank_summary(clean_text, num_sum_sent, lang)\n",
    "pagerank_sum"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Important about using PageRank!\n",
    "\n",
    "- TextRank generally works better for longer content, as it is able to identify correlation between sentences using graph linkages\n",
    "- For a shorter text, the sentence would be fewer, thus the network/linkages would be smaller as well\n",
    "- It works best for: research paper, Wikipedia page, collection of writings, etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROUGE-N Measuring the Performance of Text Summarization \n",
    "\n",
    "Recall-Oriented Understudy for Gisting Evaluation (ROUGE) is a mtehod to measure the performance of text summary by comparing the number of shared terms between the benchmark summary and the summary generated by these previous algorithms. N on ROUGE-N stands for the number of common n-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Paris Saint-Germain has put contract negotiations with Lionel Messi and Sergio Ramos on hold due to concerns that further Financial Fair Play (FFP) breaches could lead to UEFA sanctions. The club faces possible sanctions, including the inability to register new players and a reduction in the size of their registered squad for European competitions, which could impact their Champions League status.']\n"
     ]
    }
   ],
   "source": [
    "# create a text summarization for the benchmark\n",
    "# for the convenience purpose, i used summary from chatgpt\n",
    "\n",
    "with open('activity-7_bench-sum.txt') as f:\n",
    "    bench_sum = f.readlines()\n",
    "\n",
    "print(bench_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfidf_sum {'rouge1': Score(precision=0.375, recall=0.6290322580645161, fmeasure=0.4698795180722892)}\n",
      "pagerank_sum {'rouge1': Score(precision=0.42105263157894735, recall=0.6451612903225806, fmeasure=0.5095541401273885)}\n",
      "lsa_sum {'rouge1': Score(precision=0.4024390243902439, recall=0.532258064516129, fmeasure=0.4583333333333333)}\n"
     ]
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "for i, sum in zip(['tfidf_sum', 'pagerank_sum', 'lsa_sum'], [tfidf_sum, pagerank_sum, lsa_sum]):\n",
    "    summary = ' '.join(sum)\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1'], use_stemmer=True)  # define the scorer first\n",
    "    scores = scorer.score(str(bench_sum), summary)  # then input the parameters to score method\n",
    "    print(i, scores)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Important! Variations of ROUGE-N\n",
    "\n",
    "- ROUGE-L: Measures the number of common squences between the reference and generated summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on: Summarizing Text Using Machine Learning (ML) Method\n",
    "\n",
    "For this one, I will make a text analysis based on thread from Online Forum Discussion based on Tarnpradab et al. research. For the raw data set, please refer thorugh <a href='https://www.dropbox.com/s/dcds423fl7fscow/threadDataSet.zip'>this link</a>. This analysis will **summarize all the threads into a 2-3 sentences summary using Machine Learning method**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "699"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list all xml files from the dataset directory\n",
    "\n",
    "path = \"dataset/activity-7_threadDataSet/threads as original xml/\"\n",
    "dir_file = []\n",
    "\n",
    "for (root, dirs, file) in os.walk(path):\n",
    "    for f in file:\n",
    "        if '.xml' in f:\n",
    "            f_path = os.path.join(root, f)\n",
    "            dir_file.append(f_path)\n",
    "\n",
    "len(dir_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error in data: dataset/activity-7_threadDataSet/threads as original xml/batch_one/60763_5_1613009.xml\n",
      "error in data: dataset/activity-7_threadDataSet/threads as original xml/batch_one/60763_5_693940.xml\n",
      "error in data: dataset/activity-7_threadDataSet/threads as original xml/batch_one/60763_5_1439149.xml\n",
      "error in data: dataset/activity-7_threadDataSet/threads as original xml/batch_one/60763_5_1802405.xml\n",
      "error in data: dataset/activity-7_threadDataSet/threads as original xml/batch_one/60763_5_871889.xml\n",
      "error in data: dataset/activity-7_threadDataSet/threads as original xml/batch_one/60763_5_1521254.xml\n",
      "error in data: dataset/activity-7_threadDataSet/threads as original xml/batch_one/60763_5_1353950.xml\n",
      "error in data: dataset/activity-7_threadDataSet/threads as original xml/batch_two/60763_5_1107220.xml\n",
      "error in data: dataset/activity-7_threadDataSet/threads as original xml/batch_two/60763_5_1606932.xml\n",
      "error in data: dataset/activity-7_threadDataSet/threads as original xml/batch_two/60763_5_1561863.xml\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "\n",
    "for f in dir_file:\n",
    "    \n",
    "    try:\n",
    "        data = pd.read_xml(f)\n",
    "    \n",
    "        data['ThreadID'] = data.loc[0, 'ThreadID']\n",
    "        data['Title'] = data.loc[1, 'Title']\n",
    "        data.loc[2, 'rcontent'] = data.loc[2, 'icontent']\n",
    "    \n",
    "        data.rename(columns={'rcontent': 'text'}, inplace=True)\n",
    "        data.drop(index=[0,1], columns='icontent', inplace=True)\n",
    "        data.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "        data['postNum'] = data.index + 1\n",
    "        data[['Date', 'ThreadID', 'Title', 'postNum', 'text', 'UserID']]\n",
    "        df = pd.concat([df, data])\n",
    "    \n",
    "    except:\n",
    "        print('error in data:', f)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ThreadID</th>\n",
       "      <th>Title</th>\n",
       "      <th>UserID</th>\n",
       "      <th>Date</th>\n",
       "      <th>text</th>\n",
       "      <th>postNum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60763_5_666188</td>\n",
       "      <td>New York Trip Day 2</td>\n",
       "      <td>twixbar</td>\n",
       "      <td>18 June 2006, 6:02</td>\n",
       "      <td>So even though we were exhausted and had gone ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>60763_5_666188</td>\n",
       "      <td>New York Trip Day 2</td>\n",
       "      <td>SummerShowers...</td>\n",
       "      <td>18 June 2006, 19:05</td>\n",
       "      <td>Day Two really was a perfect day! Sorry your f...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60763_5_666188</td>\n",
       "      <td>New York Trip Day 2</td>\n",
       "      <td>Daisiegee</td>\n",
       "      <td>18 June 2006, 21:34</td>\n",
       "      <td>You've got me hooked......</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60763_5_666188</td>\n",
       "      <td>New York Trip Day 2</td>\n",
       "      <td>twixbar</td>\n",
       "      <td>19 June 2006, 2:57</td>\n",
       "      <td>I'm glad we went to the game too. It's stil ha...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60763_5_912589</td>\n",
       "      <td>Construction Work Near Casablanca</td>\n",
       "      <td>trollking</td>\n",
       "      <td>19 December 2006, 1:11</td>\n",
       "      <td>Any info on the construction work near the Cas...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ThreadID                              Title            UserID  \\\n",
       "0  60763_5_666188                New York Trip Day 2           twixbar   \n",
       "1  60763_5_666188                New York Trip Day 2  SummerShowers...   \n",
       "2  60763_5_666188                New York Trip Day 2         Daisiegee   \n",
       "3  60763_5_666188                New York Trip Day 2           twixbar   \n",
       "0  60763_5_912589  Construction Work Near Casablanca         trollking   \n",
       "\n",
       "                     Date                                               text  \\\n",
       "0      18 June 2006, 6:02  So even though we were exhausted and had gone ...   \n",
       "1     18 June 2006, 19:05  Day Two really was a perfect day! Sorry your f...   \n",
       "2     18 June 2006, 21:34                         You've got me hooked......   \n",
       "3      19 June 2006, 2:57  I'm glad we went to the game too. It's stil ha...   \n",
       "0  19 December 2006, 1:11  Any info on the construction work near the Cas...   \n",
       "\n",
       "   postNum  \n",
       "0        1  \n",
       "1        2  \n",
       "2        3  \n",
       "3        4  \n",
       "0        1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of the dataframe: 7234\n"
     ]
    }
   ],
   "source": [
    "display(df.head(5))\n",
    "print('length of the dataframe:', len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all xml files from the dataset directory\n",
    "\n",
    "path = \"dataset/activity-7_threadDataSet/human summaries/\"\n",
    "dir_file = []\n",
    "\n",
    "for (root, dirs, file) in os.walk(path, topdown=True):\n",
    "    dirs[:] = [d for d in dirs if d not in ['batch_one_Annotator_Two', 'batch_two_Annotator_Two', 'gold_Annotator_Two']]\n",
    "    for f in file:\n",
    "        if '.txt' in f:\n",
    "            f_path = os.path.join(root, f)\n",
    "            dir_file.append(f_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error in data: dataset/activity-7_threadDataSet/human summaries/gold_Annotator_One/60763_5_3056374.txt\n",
      "error in data: dataset/activity-7_threadDataSet/human summaries/gold_Annotator_One/60763_5_3155258.txt\n",
      "error in data: dataset/activity-7_threadDataSet/human summaries/gold_Annotator_One/60974_588_2410400.txt\n"
     ]
    }
   ],
   "source": [
    "sum_text = {}\n",
    "pattern = r\"/([^/]+)\\.txt$\"\n",
    "\n",
    "for f in dir_file:\n",
    "    try:\n",
    "        with open(f) as t:\n",
    "            text = t.read().replace('\\n', ' ')\n",
    "            match = re.search(pattern, f)\n",
    "            \n",
    "            if match:\n",
    "                file_name = match.group(1)\n",
    "                sum_text[file_name] = text\n",
    "                    \n",
    "    except:\n",
    "       print('error in data:', f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'User twixbar posted a long trip report on their second day in New York. On three hours of sleep, woke up and went to the French Roast for breakfast, a nice local place. Didn’t rush them at all. Took a cab to B&H, a huge photography store. Next took a cab to Battery Park, used pre-purchased tickets to take the ferry to the Statue of Liberty, took a tour inside the Statue. Quickly explored Ellis Island, hailed a cab to the Ed Sullivan Theater for the David Letterman Show. Got to sit up front because they looked enthusiastic, ate at the Hello Deli. Brittany Spears was the surprise guest, and Kurt Russel and a group called the Little Willies (lead singer is Norah Jones) also played. Her friend had purchased tickets to the Yankee-Red Socks game, but lost them. They managed to get tickets again from a ticket agency, went to the game and had great seats, but the friend stayed behind. After the game, wandered home down a different street, took pictures at Rockefeller Center and walked past Saks Fifth Avenue.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of the summary: 696\n"
     ]
    }
   ],
   "source": [
    "display(sum_text['60763_5_666188'])\n",
    "print('length of the summary:', len(sum_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ThreadID</th>\n",
       "      <th>Title</th>\n",
       "      <th>UserID</th>\n",
       "      <th>Date</th>\n",
       "      <th>text</th>\n",
       "      <th>postNum</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60763_5_666188</td>\n",
       "      <td>New York Trip Day 2</td>\n",
       "      <td>twixbar</td>\n",
       "      <td>18 June 2006, 6:02</td>\n",
       "      <td>So even though we were exhausted and had gone ...</td>\n",
       "      <td>1</td>\n",
       "      <td>User twixbar posted a long trip report on thei...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>60763_5_666188</td>\n",
       "      <td>New York Trip Day 2</td>\n",
       "      <td>SummerShowers...</td>\n",
       "      <td>18 June 2006, 19:05</td>\n",
       "      <td>Day Two really was a perfect day! Sorry your f...</td>\n",
       "      <td>2</td>\n",
       "      <td>User twixbar posted a long trip report on thei...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60763_5_666188</td>\n",
       "      <td>New York Trip Day 2</td>\n",
       "      <td>Daisiegee</td>\n",
       "      <td>18 June 2006, 21:34</td>\n",
       "      <td>You've got me hooked......</td>\n",
       "      <td>3</td>\n",
       "      <td>User twixbar posted a long trip report on thei...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60763_5_666188</td>\n",
       "      <td>New York Trip Day 2</td>\n",
       "      <td>twixbar</td>\n",
       "      <td>19 June 2006, 2:57</td>\n",
       "      <td>I'm glad we went to the game too. It's stil ha...</td>\n",
       "      <td>4</td>\n",
       "      <td>User twixbar posted a long trip report on thei...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60763_5_912589</td>\n",
       "      <td>Construction Work Near Casablanca</td>\n",
       "      <td>trollking</td>\n",
       "      <td>19 December 2006, 1:11</td>\n",
       "      <td>Any info on the construction work near the Cas...</td>\n",
       "      <td>1</td>\n",
       "      <td>User trollking asked about the construction wo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ThreadID                              Title            UserID  \\\n",
       "0  60763_5_666188                New York Trip Day 2           twixbar   \n",
       "1  60763_5_666188                New York Trip Day 2  SummerShowers...   \n",
       "2  60763_5_666188                New York Trip Day 2         Daisiegee   \n",
       "3  60763_5_666188                New York Trip Day 2           twixbar   \n",
       "4  60763_5_912589  Construction Work Near Casablanca         trollking   \n",
       "\n",
       "                     Date                                               text  \\\n",
       "0      18 June 2006, 6:02  So even though we were exhausted and had gone ...   \n",
       "1     18 June 2006, 19:05  Day Two really was a perfect day! Sorry your f...   \n",
       "2     18 June 2006, 21:34                         You've got me hooked......   \n",
       "3      19 June 2006, 2:57  I'm glad we went to the game too. It's stil ha...   \n",
       "4  19 December 2006, 1:11  Any info on the construction work near the Cas...   \n",
       "\n",
       "   postNum                                            summary  \n",
       "0        1  User twixbar posted a long trip report on thei...  \n",
       "1        2  User twixbar posted a long trip report on thei...  \n",
       "2        3  User twixbar posted a long trip report on thei...  \n",
       "3        4  User twixbar posted a long trip report on thei...  \n",
       "4        1  User trollking asked about the construction wo...  "
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_text = pd.DataFrame.from_dict(sum_text, orient='index')\n",
    "df = df.merge(sum_text, left_on='ThreadID', right_index=True)\n",
    "df.rename(columns={0: 'summary'}, inplace=True)\n",
    "\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total threads available for analysis: 686\n"
     ]
    }
   ],
   "source": [
    "print('total threads available for analysis:', df['ThreadID'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Creating Target Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ThreadID</th>\n",
       "      <td>60763_5_666188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Title</th>\n",
       "      <td>New York Trip Day 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UserID</th>\n",
       "      <td>twixbar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <td>18 June 2006, 6:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text</th>\n",
       "      <td>So even though we were exhausted and had gone ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>postNum</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>summary</th>\n",
       "      <td>User twixbar posted a long trip report on thei...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                          0\n",
       "ThreadID                                     60763_5_666188\n",
       "Title                                   New York Trip Day 2\n",
       "UserID                                              twixbar\n",
       "Date                                     18 June 2006, 6:02\n",
       "text      So even though we were exhausted and had gone ...\n",
       "postNum                                                   1\n",
       "summary   User twixbar posted a long trip report on thei..."
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take a look at one thread for example\n",
    "\n",
    "df[df['ThreadID'] == '60763_5_666188'].head(1).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Text Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing text using the functions we built before\n",
    "\n",
    "import fun_nlp_spacy_text as pp_text\n",
    "\n",
    "df['text'] = df['text'].apply(pp_text.clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7210/7210 [02:04<00:00, 57.80it/s]\n"
     ]
    }
   ],
   "source": [
    "# extract lemmatized version of the text\n",
    "# and also the version with only nav\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "pos_to_take = ['NOUN', 'PROPN', 'ADJ', 'ADV', 'VERB']\n",
    "\n",
    "for i, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "    doc = nlp(str(row['text']))\n",
    "    df.at[i, 'lemmas'] = ' '.join([token.lemma_ for token in doc])  # lemmatized version\n",
    "    df.at[i, 'nav'] = ' '.join([token.lemma_ for token in doc if token.pos_ in pos_to_take])  # noun-adjective-verb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ThreadID</th>\n",
       "      <th>Title</th>\n",
       "      <th>UserID</th>\n",
       "      <th>Date</th>\n",
       "      <th>text</th>\n",
       "      <th>postNum</th>\n",
       "      <th>summary</th>\n",
       "      <th>lemmas</th>\n",
       "      <th>nav</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60763_5_666188</td>\n",
       "      <td>New York Trip Day 2</td>\n",
       "      <td>twixbar</td>\n",
       "      <td>18 June 2006, 6:02</td>\n",
       "      <td>So even though we were exhausted and had gone ...</td>\n",
       "      <td>1</td>\n",
       "      <td>User twixbar posted a long trip report on thei...</td>\n",
       "      <td>so even though we be exhausted and have go to ...</td>\n",
       "      <td>so even exhausted go bed New York Time only ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>60763_5_666188</td>\n",
       "      <td>New York Trip Day 2</td>\n",
       "      <td>SummerShowers...</td>\n",
       "      <td>18 June 2006, 19:05</td>\n",
       "      <td>Day Two really was a perfect day! Sorry your f...</td>\n",
       "      <td>2</td>\n",
       "      <td>User twixbar posted a long trip report on thei...</td>\n",
       "      <td>day two really be a perfect day ! sorry your f...</td>\n",
       "      <td>day really perfect day friend join Yankees rea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60763_5_666188</td>\n",
       "      <td>New York Trip Day 2</td>\n",
       "      <td>Daisiegee</td>\n",
       "      <td>18 June 2006, 21:34</td>\n",
       "      <td>You've got me hooked......</td>\n",
       "      <td>3</td>\n",
       "      <td>User twixbar posted a long trip report on thei...</td>\n",
       "      <td>you 've get I hook ......</td>\n",
       "      <td>get hook</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60763_5_666188</td>\n",
       "      <td>New York Trip Day 2</td>\n",
       "      <td>twixbar</td>\n",
       "      <td>19 June 2006, 2:57</td>\n",
       "      <td>I'm glad we went to the game too. It's stil ha...</td>\n",
       "      <td>4</td>\n",
       "      <td>User twixbar posted a long trip report on thei...</td>\n",
       "      <td>I be glad we go to the game too . it be stil h...</td>\n",
       "      <td>glad go game too stil hard ot believe Yankee S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60763_5_912589</td>\n",
       "      <td>Construction Work Near Casablanca</td>\n",
       "      <td>trollking</td>\n",
       "      <td>19 December 2006, 1:11</td>\n",
       "      <td>Any info on the construction work near the Cas...</td>\n",
       "      <td>1</td>\n",
       "      <td>User trollking asked about the construction wo...</td>\n",
       "      <td>any info on the construction work near the Cas...</td>\n",
       "      <td>info construction work Casablanca hotel mentio...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ThreadID                              Title            UserID  \\\n",
       "0  60763_5_666188                New York Trip Day 2           twixbar   \n",
       "1  60763_5_666188                New York Trip Day 2  SummerShowers...   \n",
       "2  60763_5_666188                New York Trip Day 2         Daisiegee   \n",
       "3  60763_5_666188                New York Trip Day 2           twixbar   \n",
       "4  60763_5_912589  Construction Work Near Casablanca         trollking   \n",
       "\n",
       "                     Date                                               text  \\\n",
       "0      18 June 2006, 6:02  So even though we were exhausted and had gone ...   \n",
       "1     18 June 2006, 19:05  Day Two really was a perfect day! Sorry your f...   \n",
       "2     18 June 2006, 21:34                         You've got me hooked......   \n",
       "3      19 June 2006, 2:57  I'm glad we went to the game too. It's stil ha...   \n",
       "4  19 December 2006, 1:11  Any info on the construction work near the Cas...   \n",
       "\n",
       "   postNum                                            summary  \\\n",
       "0        1  User twixbar posted a long trip report on thei...   \n",
       "1        2  User twixbar posted a long trip report on thei...   \n",
       "2        3  User twixbar posted a long trip report on thei...   \n",
       "3        4  User twixbar posted a long trip report on thei...   \n",
       "4        1  User trollking asked about the construction wo...   \n",
       "\n",
       "                                              lemmas  \\\n",
       "0  so even though we be exhausted and have go to ...   \n",
       "1  day two really be a perfect day ! sorry your f...   \n",
       "2                          you 've get I hook ......   \n",
       "3  I be glad we go to the game too . it be stil h...   \n",
       "4  any info on the construction work near the Cas...   \n",
       "\n",
       "                                                 nav  \n",
       "0  so even exhausted go bed New York Time only ha...  \n",
       "1  day really perfect day friend join Yankees rea...  \n",
       "2                                           get hook  \n",
       "3  glad go game too stil hard ot believe Yankee S...  \n",
       "4  info construction work Casablanca hotel mentio...  "
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting between train and test split\n",
    "# using group shuffle split, because it's grouped by the thread id\n",
    "\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.2)\n",
    "train_split, test_split = next(gss.split(df, groups=df['ThreadID']))\n",
    "\n",
    "train_df = df.iloc[train_split]\n",
    "test_df = df.iloc[test_split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of threads for train: 548\n",
      "number of threads for test: 138\n"
     ]
    }
   ],
   "source": [
    "print('number of threads for train:', train_df['ThreadID'].nunique())\n",
    "print('number of threads for test:', test_df['ThreadID'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Determining Target Label for Each Post\n",
    "\n",
    "Determining whether a post in a thread should be included in the summary or not by calculating similarity between the text and picking the posts that are most similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3z/jblqnqjn04ldn_gk07kslfp00000gn/T/ipykernel_1493/545186329.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df['similarity'] = train_df.apply(\n",
      "/var/folders/3z/jblqnqjn04ldn_gk07kslfp00000gn/T/ipykernel_1493/545186329.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df['rank'] = train_df.groupby('ThreadID')['similarity'].rank(\n",
      "/var/folders/3z/jblqnqjn04ldn_gk07kslfp00000gn/T/ipykernel_1493/545186329.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df['summaryPost'] = train_df.groupby('ThreadID')['rank'].apply(topN)\n"
     ]
    }
   ],
   "source": [
    "import textdistance  # for calculating similarity between text\n",
    "\n",
    "compression_factor = 0.3  # how many sentences from a thread do we want to include for the summary, in percentage\n",
    "\n",
    "train_df['similarity'] = train_df.apply(\n",
    "    lambda x: textdistance.jaro_winkler(x.text, x.summary), axis=1)  # using jaro winkler, but there's other methods we can use\n",
    "train_df['rank'] = train_df.groupby('ThreadID')['similarity'].rank(\n",
    "    'max', ascending=False)\n",
    "\n",
    "topN = lambda x: x <= np.ceil(compression_factor * x.max())\n",
    "train_df['summaryPost'] = train_df.groupby('ThreadID')['rank'].apply(topN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>summary</th>\n",
       "      <th>summaryPost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Thanks again Cockle and Yea no probs Dublin, w...</td>\n",
       "      <td>User trollking asked about the construction wo...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>I've never stayed at the Casablanca, but many ...</td>\n",
       "      <td>User trollking asked about the construction wo...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>I stayed at the Casablanca Thursday night and ...</td>\n",
       "      <td>User trollking asked about the construction wo...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>When do we get the trip report??? ;)</td>\n",
       "      <td>User trollking asked about the construction wo...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>The trip report shall be done this evening.</td>\n",
       "      <td>User trollking asked about the construction wo...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  \\\n",
       "14  Thanks again Cockle and Yea no probs Dublin, w...   \n",
       "7   I've never stayed at the Casablanca, but many ...   \n",
       "10  I stayed at the Casablanca Thursday night and ...   \n",
       "18               When do we get the trip report??? ;)   \n",
       "20        The trip report shall be done this evening.   \n",
       "\n",
       "                                              summary  summaryPost  \n",
       "14  User trollking asked about the construction wo...         True  \n",
       "7   User trollking asked about the construction wo...         True  \n",
       "10  User trollking asked about the construction wo...         True  \n",
       "18  User trollking asked about the construction wo...        False  \n",
       "20  User trollking asked about the construction wo...        False  "
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check 5 data\n",
    "\n",
    "train_df[['text', 'summary', 'summaryPost']][train_df['ThreadID'] == '60763_5_912589'].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eliminate post that contains no more than 20 string \n",
    "# might be only noises\n",
    "\n",
    "train_df.loc[train_df['text'].str.len() <= 20, 'summaryPost'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Adding Features to Assist Model Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Adding similarity between post and the title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3z/jblqnqjn04ldn_gk07kslfp00000gn/T/ipykernel_1493/2645213878.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df['titleSimilarity'] = train_df.apply(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ThreadID</th>\n",
       "      <th>Title</th>\n",
       "      <th>UserID</th>\n",
       "      <th>Date</th>\n",
       "      <th>text</th>\n",
       "      <th>postNum</th>\n",
       "      <th>summary</th>\n",
       "      <th>lemmas</th>\n",
       "      <th>nav</th>\n",
       "      <th>similarity</th>\n",
       "      <th>rank</th>\n",
       "      <th>summaryPost</th>\n",
       "      <th>titleSimilarity</th>\n",
       "      <th>textLength</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60763_5_912589</td>\n",
       "      <td>Construction Work Near Casablanca</td>\n",
       "      <td>trollking</td>\n",
       "      <td>19 December 2006, 1:11</td>\n",
       "      <td>Any info on the construction work near the Cas...</td>\n",
       "      <td>1</td>\n",
       "      <td>User trollking asked about the construction wo...</td>\n",
       "      <td>any info on the construction work near the Cas...</td>\n",
       "      <td>info construction work Casablanca hotel mentio...</td>\n",
       "      <td>0.581996</td>\n",
       "      <td>11.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.535753</td>\n",
       "      <td>182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>60763_5_912589</td>\n",
       "      <td>Construction Work Near Casablanca</td>\n",
       "      <td>Rosie12</td>\n",
       "      <td>19 December 2006, 1:39</td>\n",
       "      <td>We were staying at the Casablanca from 24 nov ...</td>\n",
       "      <td>2</td>\n",
       "      <td>User trollking asked about the construction wo...</td>\n",
       "      <td>we be stay at the Casablanca from 24 nov to 28...</td>\n",
       "      <td>stay Casablanca nov nov construction work dist...</td>\n",
       "      <td>0.630352</td>\n",
       "      <td>8.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.516112</td>\n",
       "      <td>334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>60763_5_912589</td>\n",
       "      <td>Construction Work Near Casablanca</td>\n",
       "      <td>trollking</td>\n",
       "      <td>19 December 2006, 19:22</td>\n",
       "      <td>I have emailed the Casablanca and they have sa...</td>\n",
       "      <td>3</td>\n",
       "      <td>User trollking asked about the construction wo...</td>\n",
       "      <td>I have email the Casablanca and they have say ...</td>\n",
       "      <td>email Casablanca say know long construction go...</td>\n",
       "      <td>0.626002</td>\n",
       "      <td>10.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.512962</td>\n",
       "      <td>311</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ThreadID                              Title     UserID  \\\n",
       "4  60763_5_912589  Construction Work Near Casablanca  trollking   \n",
       "5  60763_5_912589  Construction Work Near Casablanca    Rosie12   \n",
       "6  60763_5_912589  Construction Work Near Casablanca  trollking   \n",
       "\n",
       "                      Date                                               text  \\\n",
       "4   19 December 2006, 1:11  Any info on the construction work near the Cas...   \n",
       "5   19 December 2006, 1:39  We were staying at the Casablanca from 24 nov ...   \n",
       "6  19 December 2006, 19:22  I have emailed the Casablanca and they have sa...   \n",
       "\n",
       "   postNum                                            summary  \\\n",
       "4        1  User trollking asked about the construction wo...   \n",
       "5        2  User trollking asked about the construction wo...   \n",
       "6        3  User trollking asked about the construction wo...   \n",
       "\n",
       "                                              lemmas  \\\n",
       "4  any info on the construction work near the Cas...   \n",
       "5  we be stay at the Casablanca from 24 nov to 28...   \n",
       "6  I have email the Casablanca and they have say ...   \n",
       "\n",
       "                                                 nav  similarity  rank  \\\n",
       "4  info construction work Casablanca hotel mentio...    0.581996  11.0   \n",
       "5  stay Casablanca nov nov construction work dist...    0.630352   8.0   \n",
       "6  email Casablanca say know long construction go...    0.626002  10.0   \n",
       "\n",
       "   summaryPost  titleSimilarity  textLength  \n",
       "4        False         0.535753         182  \n",
       "5        False         0.516112         334  \n",
       "6        False         0.512962         311  "
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['titleSimilarity'] = train_df.apply(\n",
    "    lambda x: textdistance.jaro_winkler(x.text, x.Title), axis=1\n",
    ")\n",
    "\n",
    "train_df.head(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Length of the post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3z/jblqnqjn04ldn_gk07kslfp00000gn/T/ipykernel_1493/2485286282.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df['textLength'] = train_df['text'].str.len()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ThreadID</th>\n",
       "      <th>Title</th>\n",
       "      <th>UserID</th>\n",
       "      <th>Date</th>\n",
       "      <th>text</th>\n",
       "      <th>postNum</th>\n",
       "      <th>summary</th>\n",
       "      <th>lemmas</th>\n",
       "      <th>nav</th>\n",
       "      <th>similarity</th>\n",
       "      <th>rank</th>\n",
       "      <th>summaryPost</th>\n",
       "      <th>titleSimilarity</th>\n",
       "      <th>textLength</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60763_5_912589</td>\n",
       "      <td>Construction Work Near Casablanca</td>\n",
       "      <td>trollking</td>\n",
       "      <td>19 December 2006, 1:11</td>\n",
       "      <td>Any info on the construction work near the Cas...</td>\n",
       "      <td>1</td>\n",
       "      <td>User trollking asked about the construction wo...</td>\n",
       "      <td>any info on the construction work near the Cas...</td>\n",
       "      <td>info construction work Casablanca hotel mentio...</td>\n",
       "      <td>0.581996</td>\n",
       "      <td>11.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.535753</td>\n",
       "      <td>182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>60763_5_912589</td>\n",
       "      <td>Construction Work Near Casablanca</td>\n",
       "      <td>Rosie12</td>\n",
       "      <td>19 December 2006, 1:39</td>\n",
       "      <td>We were staying at the Casablanca from 24 nov ...</td>\n",
       "      <td>2</td>\n",
       "      <td>User trollking asked about the construction wo...</td>\n",
       "      <td>we be stay at the Casablanca from 24 nov to 28...</td>\n",
       "      <td>stay Casablanca nov nov construction work dist...</td>\n",
       "      <td>0.630352</td>\n",
       "      <td>8.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.516112</td>\n",
       "      <td>334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>60763_5_912589</td>\n",
       "      <td>Construction Work Near Casablanca</td>\n",
       "      <td>trollking</td>\n",
       "      <td>19 December 2006, 19:22</td>\n",
       "      <td>I have emailed the Casablanca and they have sa...</td>\n",
       "      <td>3</td>\n",
       "      <td>User trollking asked about the construction wo...</td>\n",
       "      <td>I have email the Casablanca and they have say ...</td>\n",
       "      <td>email Casablanca say know long construction go...</td>\n",
       "      <td>0.626002</td>\n",
       "      <td>10.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.512962</td>\n",
       "      <td>311</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ThreadID                              Title     UserID  \\\n",
       "4  60763_5_912589  Construction Work Near Casablanca  trollking   \n",
       "5  60763_5_912589  Construction Work Near Casablanca    Rosie12   \n",
       "6  60763_5_912589  Construction Work Near Casablanca  trollking   \n",
       "\n",
       "                      Date                                               text  \\\n",
       "4   19 December 2006, 1:11  Any info on the construction work near the Cas...   \n",
       "5   19 December 2006, 1:39  We were staying at the Casablanca from 24 nov ...   \n",
       "6  19 December 2006, 19:22  I have emailed the Casablanca and they have sa...   \n",
       "\n",
       "   postNum                                            summary  \\\n",
       "4        1  User trollking asked about the construction wo...   \n",
       "5        2  User trollking asked about the construction wo...   \n",
       "6        3  User trollking asked about the construction wo...   \n",
       "\n",
       "                                              lemmas  \\\n",
       "4  any info on the construction work near the Cas...   \n",
       "5  we be stay at the Casablanca from 24 nov to 28...   \n",
       "6  I have email the Casablanca and they have say ...   \n",
       "\n",
       "                                                 nav  similarity  rank  \\\n",
       "4  info construction work Casablanca hotel mentio...    0.581996  11.0   \n",
       "5  stay Casablanca nov nov construction work dist...    0.630352   8.0   \n",
       "6  email Casablanca say know long construction go...    0.626002  10.0   \n",
       "\n",
       "   summaryPost  titleSimilarity  textLength  \n",
       "4        False         0.535753         182  \n",
       "5        False         0.516112         334  \n",
       "6        False         0.512962         311  "
      ]
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['textLength'] = train_df['text'].str.len()\n",
    "train_df.head(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Vector of the lemmatized text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/taufiqurrohman/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10th</th>\n",
       "      <th>11th</th>\n",
       "      <th>12th</th>\n",
       "      <th>14th</th>\n",
       "      <th>14th street</th>\n",
       "      <th>15th</th>\n",
       "      <th>17th</th>\n",
       "      <th>18th</th>\n",
       "      <th>19th</th>\n",
       "      <th>1st</th>\n",
       "      <th>...</th>\n",
       "      <th>yorker</th>\n",
       "      <th>yorkers</th>\n",
       "      <th>young</th>\n",
       "      <th>yr</th>\n",
       "      <th>yr old</th>\n",
       "      <th>yummy</th>\n",
       "      <th>zabar</th>\n",
       "      <th>zero</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2951 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   10th  11th  12th  14th  14th street  15th  17th  18th  19th  1st  ...  \\\n",
       "0   0.0   0.0   0.0   0.0          0.0   0.0   0.0   0.0   0.0  0.0  ...   \n",
       "1   0.0   0.0   0.0   0.0          0.0   0.0   0.0   0.0   0.0  0.0  ...   \n",
       "2   0.0   0.0   0.0   0.0          0.0   0.0   0.0   0.0   0.0  0.0  ...   \n",
       "3   0.0   0.0   0.0   0.0          0.0   0.0   0.0   0.0   0.0  0.0  ...   \n",
       "4   0.0   0.0   0.0   0.0          0.0   0.0   0.0   0.0   0.0  0.0  ...   \n",
       "\n",
       "   yorker  yorkers  young   yr  yr old  yummy  zabar  zero  zone  zoo  \n",
       "0     0.0      0.0    0.0  0.0     0.0    0.0    0.0   0.0   0.0  0.0  \n",
       "1     0.0      0.0    0.0  0.0     0.0    0.0    0.0   0.0   0.0  0.0  \n",
       "2     0.0      0.0    0.0  0.0     0.0    0.0    0.0   0.0   0.0  0.0  \n",
       "3     0.0      0.0    0.0  0.0     0.0    0.0    0.0   0.0   0.0  0.0  \n",
       "4     0.0      0.0    0.0  0.0     0.0    0.0    0.0   0.0   0.0  0.0  \n",
       "\n",
       "[5 rows x 2951 columns]"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(min_df=10, ngram_range=(1,2), stop_words='english')\n",
    "tfidf_result = tfidf.fit_transform(train_df['nav']).toarray()  # using noun-adjective-verb, could also be done using lemmatized form\n",
    "tfidf_df = pd.DataFrame(tfidf_result, columns=tfidf.get_feature_names()) \n",
    "\n",
    "tfidf_df.head(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Inserting all the features, combine into one dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>titleSimilarity</th>\n",
       "      <th>textLength</th>\n",
       "      <th>postNum</th>\n",
       "      <th>token_10th</th>\n",
       "      <th>token_11th</th>\n",
       "      <th>token_12th</th>\n",
       "      <th>token_14th</th>\n",
       "      <th>token_14th street</th>\n",
       "      <th>token_15th</th>\n",
       "      <th>token_17th</th>\n",
       "      <th>...</th>\n",
       "      <th>token_yorker</th>\n",
       "      <th>token_yorkers</th>\n",
       "      <th>token_young</th>\n",
       "      <th>token_yr</th>\n",
       "      <th>token_yr old</th>\n",
       "      <th>token_yummy</th>\n",
       "      <th>token_zabar</th>\n",
       "      <th>token_zero</th>\n",
       "      <th>token_zone</th>\n",
       "      <th>token_zoo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.535753</td>\n",
       "      <td>182</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.516112</td>\n",
       "      <td>334</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.512962</td>\n",
       "      <td>311</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.493886</td>\n",
       "      <td>567</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.500226</td>\n",
       "      <td>515</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2954 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   titleSimilarity  textLength  postNum  token_10th  token_11th  token_12th  \\\n",
       "4         0.535753         182        1         0.0         0.0         0.0   \n",
       "5         0.516112         334        2         0.0         0.0         0.0   \n",
       "6         0.512962         311        3         0.0         0.0         0.0   \n",
       "7         0.493886         567        4         0.0         0.0         0.0   \n",
       "8         0.500226         515        5         0.0         0.0         0.0   \n",
       "\n",
       "   token_14th  token_14th street  token_15th  token_17th  ...  token_yorker  \\\n",
       "4         0.0                0.0         0.0         0.0  ...           0.0   \n",
       "5         0.0                0.0         0.0         0.0  ...           0.0   \n",
       "6         0.0                0.0         0.0         0.0  ...           0.0   \n",
       "7         0.0                0.0         0.0         0.0  ...           0.0   \n",
       "8         0.0                0.0         0.0         0.0  ...           0.0   \n",
       "\n",
       "   token_yorkers  token_young  token_yr  token_yr old  token_yummy  \\\n",
       "4            0.0          0.0       0.0           0.0          0.0   \n",
       "5            0.0          0.0       0.0           0.0          0.0   \n",
       "6            0.0          0.0       0.0           0.0          0.0   \n",
       "7            0.0          0.0       0.0           0.0          0.0   \n",
       "8            0.0          0.0       0.0           0.0          0.0   \n",
       "\n",
       "   token_zabar  token_zero  token_zone  token_zoo  \n",
       "4          0.0         0.0         0.0        0.0  \n",
       "5          0.0         0.0         0.0        0.0  \n",
       "6          0.0         0.0         0.0        0.0  \n",
       "7          0.0         0.0         0.0        0.0  \n",
       "8          0.0         0.0         0.0        0.0  \n",
       "\n",
       "[5 rows x 2954 columns]"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# renaming columns of tfidf with token, to show that this is weight of each words\n",
    "tfidf_df.columns = ['token_' + str(x) for x in tfidf_df.columns]\n",
    "tfidf_df.index = train_df.index\n",
    "\n",
    "# add post num, might be also explaining whether it should be included in the summary\n",
    "feature_cols = ['titleSimilarity', 'textLength', 'postNum']  \n",
    "train_df_tf = pd.concat([train_df[feature_cols], tfidf_df], axis=1)\n",
    "train_df_tf.head(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One that missing! Make the dataframe for test_df!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3z/jblqnqjn04ldn_gk07kslfp00000gn/T/ipykernel_1493/1289872086.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df['similarity'] = test_df.apply(\n",
      "/var/folders/3z/jblqnqjn04ldn_gk07kslfp00000gn/T/ipykernel_1493/1289872086.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df['rank'] = test_df.groupby('ThreadID')['similarity'].rank(\n",
      "/var/folders/3z/jblqnqjn04ldn_gk07kslfp00000gn/T/ipykernel_1493/1289872086.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df['summaryPost'] = test_df.groupby('ThreadID')['rank'].apply(topN)\n",
      "/var/folders/3z/jblqnqjn04ldn_gk07kslfp00000gn/T/ipykernel_1493/1289872086.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df['titleSimilarity'] = test_df.apply(\n",
      "/var/folders/3z/jblqnqjn04ldn_gk07kslfp00000gn/T/ipykernel_1493/1289872086.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df['textLength'] = test_df['text'].str.len()\n",
      "/Users/taufiqurrohman/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>titleSimilarity</th>\n",
       "      <th>textLength</th>\n",
       "      <th>postNum</th>\n",
       "      <th>token_10th</th>\n",
       "      <th>token_11th</th>\n",
       "      <th>token_12th</th>\n",
       "      <th>token_14th</th>\n",
       "      <th>token_14th street</th>\n",
       "      <th>token_15th</th>\n",
       "      <th>token_17th</th>\n",
       "      <th>...</th>\n",
       "      <th>token_yorker</th>\n",
       "      <th>token_yorkers</th>\n",
       "      <th>token_young</th>\n",
       "      <th>token_yr</th>\n",
       "      <th>token_yr old</th>\n",
       "      <th>token_yummy</th>\n",
       "      <th>token_zabar</th>\n",
       "      <th>token_zero</th>\n",
       "      <th>token_zone</th>\n",
       "      <th>token_zoo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.501797</td>\n",
       "      <td>7293</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.041082</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.497845</td>\n",
       "      <td>245</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.403027</td>\n",
       "      <td>26</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.415121</td>\n",
       "      <td>84</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.513547</td>\n",
       "      <td>3028</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.058798</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2954 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    titleSimilarity  textLength  postNum  token_10th  token_11th  token_12th  \\\n",
       "0          0.501797        7293        1         0.0         0.0         0.0   \n",
       "1          0.497845         245        2         0.0         0.0         0.0   \n",
       "2          0.403027          26        3         0.0         0.0         0.0   \n",
       "3          0.415121          84        4         0.0         0.0         0.0   \n",
       "22         0.513547        3028        1         0.0         0.0         0.0   \n",
       "\n",
       "    token_14th  token_14th street  token_15th  token_17th  ...  token_yorker  \\\n",
       "0          0.0                0.0         0.0         0.0  ...           0.0   \n",
       "1          0.0                0.0         0.0         0.0  ...           0.0   \n",
       "2          0.0                0.0         0.0         0.0  ...           0.0   \n",
       "3          0.0                0.0         0.0         0.0  ...           0.0   \n",
       "22         0.0                0.0         0.0         0.0  ...           0.0   \n",
       "\n",
       "    token_yorkers  token_young  token_yr  token_yr old  token_yummy  \\\n",
       "0             0.0          0.0       0.0           0.0     0.041082   \n",
       "1             0.0          0.0       0.0           0.0     0.000000   \n",
       "2             0.0          0.0       0.0           0.0     0.000000   \n",
       "3             0.0          0.0       0.0           0.0     0.000000   \n",
       "22            0.0          0.0       0.0           0.0     0.000000   \n",
       "\n",
       "    token_zabar  token_zero  token_zone  token_zoo  \n",
       "0           0.0    0.000000         0.0        0.0  \n",
       "1           0.0    0.000000         0.0        0.0  \n",
       "2           0.0    0.000000         0.0        0.0  \n",
       "3           0.0    0.000000         0.0        0.0  \n",
       "22          0.0    0.058798         0.0        0.0  \n",
       "\n",
       "[5 rows x 2954 columns]"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### similarity to the summary\n",
    "\n",
    "compression_factor = 0.3  # how many sentences from a thread do we want to include for the summary, in percentage\n",
    "\n",
    "test_df['similarity'] = test_df.apply(\n",
    "    lambda x: textdistance.jaro_winkler(x.text, x.summary), axis=1)  # using jaro winkler, but there's other methods we can use\n",
    "test_df['rank'] = test_df.groupby('ThreadID')['similarity'].rank(\n",
    "    'max', ascending=False)\n",
    "\n",
    "topN = lambda x: x <= np.ceil(compression_factor * x.max())\n",
    "test_df['summaryPost'] = test_df.groupby('ThreadID')['rank'].apply(topN)\n",
    "\n",
    "test_df.loc[test_df['text'].str.len() <= 20, 'summaryPost'] = False\n",
    "\n",
    "\n",
    "### feature engineering\n",
    "\n",
    "test_df['titleSimilarity'] = test_df.apply(\n",
    "    lambda x: textdistance.jaro_winkler(x.text, x.Title), axis=1\n",
    ")\n",
    "\n",
    "test_df['textLength'] = test_df['text'].str.len()\n",
    "\n",
    "tfidf_result = tfidf.transform(test_df['nav']).toarray()  # IMPORTANT, for test, instead of fit_transform(), we use transform()\n",
    "tfidf_df = pd.DataFrame(tfidf_result, columns=tfidf.get_feature_names()) \n",
    "\n",
    "\n",
    "### inserting all features into a dataframe\n",
    "\n",
    "tfidf_df.columns = ['token_' + str(x) for x in tfidf_df.columns]\n",
    "tfidf_df.index = test_df.index\n",
    "\n",
    "feature_cols = ['titleSimilarity', 'textLength', 'postNum']  \n",
    "test_df_tf = pd.concat([test_df[feature_cols], tfidf_df], axis=1)\n",
    "test_df_tf.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Build the Machine Learning Model\n",
    "\n",
    "Build the model using the RandomForestClassifier; tree-based algorithm might perform better for a data with number + categorical features combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3z/jblqnqjn04ldn_gk07kslfp00000gn/T/ipykernel_1493/4196325198.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df['summaryPost_predicted'] = model.predict(test_df_tf)\n"
     ]
    }
   ],
   "source": [
    "# build the ml model and fit it to the test model\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "x = train_df_tf\n",
    "y = train_df['summaryPost']\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "model.fit(x, y)\n",
    "\n",
    "test_df['summaryPost_predicted'] = model.predict(test_df_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a function to calculate rouge score\n",
    "\n",
    "def calculate_rouge_score(x, column_name):\n",
    "    ref_summary = x['summary'].values[0]\n",
    "    \n",
    "    predicted_summary = ''.join(x['text'][x[column_name]])\n",
    "    \n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1'], use_stemmer=True)\n",
    "    scores = scorer.score(ref_summary, predicted_summary)\n",
    "    return scores['rouge1'].fmeasure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge score for test: 0.35351810647553134\n"
     ]
    }
   ],
   "source": [
    "rouge_score = test_df.groupby('ThreadID')[['summary', 'text', 'summaryPost_predicted']].\\\n",
    "    apply(calculate_rouge_score, column_name='summaryPost_predicted').mean()\n",
    "    \n",
    "print('rouge score for test:', rouge_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A woman wanted to spend 2 hours with the kids in Niagara. She felt spending time at McDonald was a good idea but still needed suggestions to spend time. Someone suggested visiting the zoo which had elephants. The other places suggested were a museum of Science, beautiful collection of greenhouses.  Some other person suggests her to visit Aquatic and Fitness Center on Delaware which had Olympic-size swimming pool and which was 15 min ride from the Darwin Martin house. The other suggested places were ToyTown museum, Roycroft campus and Vidlers. Finally she ended up at the Niagara Aquarium. \n",
      "\n",
      "Predicted Summary:\n",
      " ['Buffalo doesn\\'t have a children\\'s musuem, unfortunately. The closest one is in Rochester, NY and that\\'s a one hour drive. The Bflo zoo is an ideal location since you can park near the Darwin Martin house and walk to the zoo. They have river otters so cute! And you can see the elephants up close and personal if they\\'re not outside (it looks like it might be cold tomorrow, but they do let them out when it\\'s around freezing outside. They have a heated ramp so no worries about them slipping on the snow and ice...) There is also a Museum of Science, but if you\\'re used to the museums in Chicago, then this one won\\'t compare. tripadvisor.com/Attraction_Review-g60974-d26… The Albright-Knox Art Gallery is not far from the D. Martin House. Not sure if your boys would find it amusing, but it has a great collection of modern art. If you wait until 3:00, museum entrance is free, and there are activities (starting around 5:00) for children and adults alike. Very fun time! If you pick up a \"Buffalo News\" and look at the \"Gusto\" insert, you\\'ll find an advt, usually in the early pages, regarding the night\\'s activities. tripadvisor.com/Attraction_Review-g60974-d26… To the south of town, in Lackawanna, NY, is the Botanical Gardens. A small, beautiful collection of greenhouses. Not a bad idea for little guys, especially if it\\'s cold outside. Down the street you\\'ll notice the Basilica, an ornate scaled down version of St. Peter\\'s in Rome. Lots of marble and stained glass to see if you care to step in. tripadvisor.com/Attraction_Review-g60974-d26… Hope you find something you like to do!'\n",
      " \"Explore and More is a children's museum in East Aurora, about 25 min south of Buffalo. How young ar eyour kids? It's great for the 6 and under. ToyTown museum is a toy museum in East Aurora and next to a Toy store on the Fisher-Price campus. If you like archetecture, visit the Roycroft campus (also in EA). Vidlers is a great store (old 5 10), kids will love it. take in a movie on Main st, Horton Hears a WHo is now playing.\"] \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>postNum</th>\n",
       "      <th>text</th>\n",
       "      <th>summaryPost</th>\n",
       "      <th>summaryPost_predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6757</th>\n",
       "      <td>1</td>\n",
       "      <td>We are in Niagara CA side and will be in Buffa...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6758</th>\n",
       "      <td>2</td>\n",
       "      <td>Buffalo doesn't have a children's musuem, unfo...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6759</th>\n",
       "      <td>3</td>\n",
       "      <td>Thank you Lady Dee. I read about the one in Ro...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6760</th>\n",
       "      <td>4</td>\n",
       "      <td>Sorry I am late....A few hours at the Galleria...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6761</th>\n",
       "      <td>5</td>\n",
       "      <td>I'm late, too, but another option, if you're i...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6762</th>\n",
       "      <td>6</td>\n",
       "      <td>Explore and More is a children's museum in Eas...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6763</th>\n",
       "      <td>7</td>\n",
       "      <td>Wow, some great suggestions! Unfortunately I d...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      postNum                                               text  summaryPost  \\\n",
       "6757        1  We are in Niagara CA side and will be in Buffa...         True   \n",
       "6758        2  Buffalo doesn't have a children's musuem, unfo...        False   \n",
       "6759        3  Thank you Lady Dee. I read about the one in Ro...        False   \n",
       "6760        4  Sorry I am late....A few hours at the Galleria...        False   \n",
       "6761        5  I'm late, too, but another option, if you're i...         True   \n",
       "6762        6  Explore and More is a children's museum in Eas...         True   \n",
       "6763        7  Wow, some great suggestions! Unfortunately I d...        False   \n",
       "\n",
       "      summaryPost_predicted  \n",
       "6757                  False  \n",
       "6758                   True  \n",
       "6759                  False  \n",
       "6760                  False  \n",
       "6761                  False  \n",
       "6762                   True  \n",
       "6763                  False  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# take a look at the data that we fit already\n",
    "\n",
    "thread_id = '60974_588_1849664'\n",
    "\n",
    "instance_df = test_df[test_df['ThreadID'] == thread_id]\n",
    "print(instance_df['summary'].iloc[0])\n",
    "\n",
    "print('\\nPredicted Summary:\\n', instance_df[instance_df['summaryPost_predicted'] == True]['text'].values, '\\n')\n",
    "\n",
    "display(instance_df[['postNum', 'text', 'summaryPost', 'summaryPost_predicted']])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0fd9779e8d8f386467a1f1d1102cd9c30d79c45b15374f3d42e7180eb33a0483"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
